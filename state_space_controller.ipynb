{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "498c9790",
   "metadata": {},
   "source": [
    "# State-Space Control and LSTM-Based Controller Design\n",
    "## Vertical Acceleration Control for Autonomous Systems\n",
    "\n",
    "This notebook implements and compares two control approaches:\n",
    "1. **Linear Quadratic Regulator (LQR)** - Classical optimal control with state-space models\n",
    "2. **LSTM-Based Controller** - Data-driven neural network controller\n",
    "\n",
    "The goal is to control vertical acceleration (and thus position/velocity) of a system with normalized command inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0071a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# IMPORTS AND SETUP\n",
    "# ============================================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.linalg\n",
    "from scipy.stats import zscore\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# System parameters\n",
    "dt = 0.05  # Sampling time (seconds)\n",
    "g = 9.81   # Gravitational acceleration (m/s^2)\n",
    "\n",
    "print(f\"System configured with dt={dt}s, g={g}m/s²\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16d1ced",
   "metadata": {},
   "source": [
    "## Section 1: Data Loading and Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2813ff8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load experimental data\n",
    "df_in = pd.read_csv('bdd_in_mat_05.csv')\n",
    "df_out = pd.read_csv('bdd_out_mat_05.csv')\n",
    "\n",
    "print(f\"Input data shape: {df_in.shape}\")\n",
    "print(f\"Output data shape: {df_out.shape}\")\n",
    "print(f\"\\nFirst 5 input values (first sample): {df_in.iloc[0, :5].values}\")\n",
    "print(f\"First 5 output values (first sample): {df_out.iloc[0, :5].values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9210e07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot raw data exploration\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# All input signals\n",
    "axes[0, 0].plot(df_in.T, alpha=0.3, color='blue')\n",
    "axes[0, 0].set_title('All Input Signals (Normalized Commands)', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Time Step')\n",
    "axes[0, 0].set_ylabel('Normalized Command')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# All output signals\n",
    "axes[0, 1].plot(df_out.T, alpha=0.3, color='green')\n",
    "axes[0, 1].set_title('All Output Signals (Acceleration)', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Time Step')\n",
    "axes[0, 1].set_ylabel('Acceleration (m/s²)')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# First sample input vs output\n",
    "time_steps = np.arange(len(df_in.iloc[0]))\n",
    "ax1 = axes[1, 0]\n",
    "ax2 = ax1.twinx()\n",
    "ax1.plot(df_in.iloc[0].values, label='Input Command', color='blue', linewidth=2)\n",
    "ax2.plot(df_out.iloc[0].values, label='Output Acceleration', color='red', linewidth=2)\n",
    "ax1.set_xlabel('Time Step')\n",
    "ax1.set_ylabel('Input (Normalized)', color='blue')\n",
    "ax2.set_ylabel('Output (m/s²)', color='red')\n",
    "axes[1, 0].set_title('First Sample: Input vs Output', fontsize=12, fontweight='bold')\n",
    "ax1.tick_params(axis='y', labelcolor='blue')\n",
    "ax2.tick_params(axis='y', labelcolor='red')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Scatter plot for correlation\n",
    "axes[1, 1].scatter(df_in.iloc[0].values, df_out.iloc[0].values, alpha=0.5, s=10)\n",
    "axes[1, 1].set_xlabel('Input Command (Normalized)')\n",
    "axes[1, 1].set_ylabel('Output Acceleration (m/s²)')\n",
    "axes[1, 1].set_title('Input-Output Correlation', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Data exploration complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c33793",
   "metadata": {},
   "source": [
    "## Section 2: Data Preprocessing and Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557e6075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier detection and removal using Z-score\n",
    "df_in_cleaned = df_in.copy()\n",
    "df_out_cleaned = df_out.copy()\n",
    "\n",
    "z_score_threshold = 3\n",
    "z_scores_in = zscore(df_in_cleaned, axis=1, nan_policy='omit')\n",
    "z_scores_out = zscore(df_out_cleaned, axis=1, nan_policy='omit')\n",
    "\n",
    "outlier_rows = (np.abs(z_scores_in) > z_score_threshold).any(axis=1) | \\\n",
    "               (np.abs(z_scores_out) > z_score_threshold).any(axis=1)\n",
    "\n",
    "rows_to_keep = ~outlier_rows\n",
    "df_in_cleaned = df_in_cleaned[rows_to_keep].reset_index(drop=True)\n",
    "df_out_cleaned = df_out_cleaned[rows_to_keep].reset_index(drop=True)\n",
    "\n",
    "print(f\"Original samples: {len(df_in)}\")\n",
    "print(f\"Outliers removed: {np.sum(outlier_rows)}\")\n",
    "print(f\"Remaining samples: {len(df_in_cleaned)}\")\n",
    "\n",
    "# Normalize input data (MinMax scaling to [0,1])\n",
    "X = df_in_cleaned.to_numpy()\n",
    "scaler_X = MinMaxScaler()\n",
    "X_scaled = scaler_X.fit_transform(X)\n",
    "\n",
    "# Normalize output data\n",
    "# Shift by initial value, then normalize by max absolute value\n",
    "y = df_out_cleaned.to_numpy()\n",
    "y_shifted = y - y[:, 0:1]\n",
    "global_max_abs_y = np.max(np.abs(y_shifted))\n",
    "y_normalized = y_shifted / global_max_abs_y\n",
    "\n",
    "print(f\"\\nInput shape: {X_scaled.shape}\")\n",
    "print(f\"Output shape: {y_normalized.shape}\")\n",
    "print(f\"Global max acceleration: {global_max_abs_y:.4f} m/s²\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099476e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape for LSTM (samples, timesteps, features)\n",
    "n_samples, n_timesteps = X_scaled.shape\n",
    "X_reshaped = X_scaled.reshape(n_samples, n_timesteps, 1)\n",
    "y_reshaped = y_normalized.reshape(n_samples, n_timesteps, 1)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_reshaped, y_reshaped, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set: X_train {X_train.shape}, y_train {y_train.shape}\")\n",
    "print(f\"Test set: X_test {X_test.shape}, y_test {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eceb8fc5",
   "metadata": {},
   "source": [
    "## Section 3: Forward LSTM Model (System Dynamics)\n",
    "\n",
    "This model learns the system dynamics: acceleration = f(command history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a37e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build forward LSTM model (predicts acceleration from commands)\n",
    "forward_model = Sequential([\n",
    "    LSTM(32, input_shape=(n_timesteps, 1), return_sequences=True, activation='relu'),\n",
    "    LSTM(16, return_sequences=True, activation='relu'),\n",
    "    Dense(1, activation='linear')  # Linear output for acceleration\n",
    "])\n",
    "\n",
    "forward_model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "print(\"Forward Model Architecture:\")\n",
    "forward_model.summary()\n",
    "\n",
    "# Train forward model\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Training Forward LSTM Model...\")\n",
    "print(\"=\"*60)\n",
    "history_forward = forward_model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=25,\n",
    "    batch_size=16,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "test_loss = forward_model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"\\nForward Model Test Loss: {test_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e59177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 4))\n",
    "\n",
    "axes[0].plot(history_forward.history['loss'], label='Training Loss', linewidth=2)\n",
    "axes[0].plot(history_forward.history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss (MSE)')\n",
    "axes[0].set_title('Forward Model: Training History', fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_yscale('log')\n",
    "\n",
    "axes[1].plot(history_forward.history['mae'], label='Training MAE', linewidth=2)\n",
    "axes[1].plot(history_forward.history['val_mae'], label='Validation MAE', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('MAE')\n",
    "axes[1].set_title('Forward Model: Mean Absolute Error', fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Forward model training complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0111689f",
   "metadata": {},
   "source": [
    "## Section 4: Forward Model Validation and Prediction Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3ff251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions on test set\n",
    "y_pred_normalized = forward_model.predict(X_test, verbose=0)\n",
    "\n",
    "# Denormalize predictions\n",
    "y_pred_real = y_pred_normalized * global_max_abs_y\n",
    "y_test_real = y_test * global_max_abs_y\n",
    "\n",
    "# Denormalize by adding back gravity baseline\n",
    "y_pred_accel = y_pred_real\n",
    "y_test_accel = y_test_real\n",
    "\n",
    "# Calculate error metrics\n",
    "mse = mean_squared_error(y_test_accel.flatten(), y_pred_accel.flatten())\n",
    "rmse = np.sqrt(mse)\n",
    "mae = np.mean(np.abs(y_test_accel.flatten() - y_pred_accel.flatten()))\n",
    "\n",
    "print(f\"Forward Model Performance Metrics:\")\n",
    "print(f\"  RMSE: {rmse:.6f} m/s²\")\n",
    "print(f\"  MAE:  {mae:.6f} m/s²\")\n",
    "print(f\"  MSE:  {mse:.6f}\")\n",
    "\n",
    "# Select a test sample for detailed analysis\n",
    "sample_idx = 2\n",
    "y_test_sample = y_test_accel[sample_idx].squeeze()\n",
    "y_pred_sample = y_pred_accel[sample_idx].squeeze()\n",
    "x_test_sample = X_test[sample_idx].squeeze()\n",
    "\n",
    "# Calculate velocity and position by integration\n",
    "def integrate_acceleration(accel, dt=0.05):\n",
    "    \"\"\"Double integrate acceleration to get velocity and position\"\"\"\n",
    "    velocity = np.cumsum(accel) * dt\n",
    "    position = np.cumsum(velocity) * dt\n",
    "    return position, velocity\n",
    "\n",
    "pos_real, vel_real = integrate_acceleration(y_test_sample, dt)\n",
    "pos_pred, vel_pred = integrate_acceleration(y_pred_sample, dt)\n",
    "\n",
    "# Create comprehensive comparison plot\n",
    "fig = plt.figure(figsize=(16, 12))\n",
    "gs = fig.add_gridspec(4, 2, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# Row 1: Command vs Acceleration\n",
    "ax1 = fig.add_subplot(gs[0, :])\n",
    "ax1_twin = ax1.twinx()\n",
    "ax1.plot(x_test_sample[:200], label='Command Input', color='blue', linewidth=2, alpha=0.7)\n",
    "ax1_twin.plot(y_test_sample[:200], label='Real Acceleration', color='red', linewidth=2, alpha=0.7)\n",
    "ax1_twin.plot(y_pred_sample[:200], label='Predicted Acceleration', color='red', linewidth=2, \n",
    "              alpha=0.4, linestyle='--')\n",
    "ax1.set_xlabel('Time Step')\n",
    "ax1.set_ylabel('Command (Normalized)', color='blue')\n",
    "ax1_twin.set_ylabel('Acceleration (m/s²)', color='red')\n",
    "ax1.set_title('Test Sample: Command vs Acceleration (Forward Model)', fontweight='bold', fontsize=11)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.tick_params(axis='y', labelcolor='blue')\n",
    "ax1_twin.tick_params(axis='y', labelcolor='red')\n",
    "fig.legend(loc='upper right', bbox_to_anchor=(0.98, 0.96), fontsize=10)\n",
    "\n",
    "# Row 2: Acceleration comparison\n",
    "ax2 = fig.add_subplot(gs[1, 0])\n",
    "ax2.plot(y_test_sample, label='Real', linewidth=2, color='green')\n",
    "ax2.plot(y_pred_sample, label='Predicted', linewidth=2, color='orange', alpha=0.7, linestyle='--')\n",
    "ax2.set_xlabel('Time Step')\n",
    "ax2.set_ylabel('Acceleration (m/s²)')\n",
    "ax2.set_title('Acceleration: Real vs Predicted', fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Acceleration error\n",
    "ax3 = fig.add_subplot(gs[1, 1])\n",
    "accel_error = np.abs(y_test_sample - y_pred_sample)\n",
    "ax3.plot(accel_error, color='red', linewidth=2, alpha=0.7)\n",
    "ax3.fill_between(range(len(accel_error)), accel_error, alpha=0.3, color='red')\n",
    "ax3.set_xlabel('Time Step')\n",
    "ax3.set_ylabel('Absolute Error (m/s²)')\n",
    "ax3.set_title(f'Acceleration Error (Mean: {mae:.4f} m/s²)', fontweight='bold')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Row 3: Velocity\n",
    "ax4 = fig.add_subplot(gs[2, 0])\n",
    "ax4.plot(vel_real, label='Real', linewidth=2, color='green')\n",
    "ax4.plot(vel_pred, label='Predicted', linewidth=2, color='orange', alpha=0.7, linestyle='--')\n",
    "ax4.set_xlabel('Time Step')\n",
    "ax4.set_ylabel('Velocity (m/s)')\n",
    "ax4.set_title('Integrated Velocity', fontweight='bold')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "# Velocity error\n",
    "ax5 = fig.add_subplot(gs[2, 1])\n",
    "vel_error = np.abs(vel_real - vel_pred)\n",
    "ax5.plot(vel_error, color='purple', linewidth=2, alpha=0.7)\n",
    "ax5.fill_between(range(len(vel_error)), vel_error, alpha=0.3, color='purple')\n",
    "ax5.set_xlabel('Time Step')\n",
    "ax5.set_ylabel('Velocity Error (m/s)')\n",
    "ax5.set_title(f'Velocity Error (Max: {np.max(vel_error):.4f} m/s)', fontweight='bold')\n",
    "ax5.grid(True, alpha=0.3)\n",
    "\n",
    "# Row 4: Position\n",
    "ax6 = fig.add_subplot(gs[3, 0])\n",
    "ax6.plot(pos_real, label='Real', linewidth=2, color='green')\n",
    "ax6.plot(pos_pred, label='Predicted', linewidth=2, color='orange', alpha=0.7, linestyle='--')\n",
    "ax6.set_xlabel('Time Step')\n",
    "ax6.set_ylabel('Position (m)')\n",
    "ax6.set_title('Integrated Position', fontweight='bold')\n",
    "ax6.legend()\n",
    "ax6.grid(True, alpha=0.3)\n",
    "\n",
    "# Position error\n",
    "ax7 = fig.add_subplot(gs[3, 1])\n",
    "pos_error = np.abs(pos_real - pos_pred)\n",
    "ax7.plot(pos_error, color='brown', linewidth=2, alpha=0.7)\n",
    "ax7.fill_between(range(len(pos_error)), pos_error, alpha=0.3, color='brown')\n",
    "ax7.set_xlabel('Time Step')\n",
    "ax7.set_ylabel('Position Error (m)')\n",
    "ax7.set_title(f'Position Error (Max: {np.max(pos_error):.4f} m)', fontweight='bold')\n",
    "ax7.grid(True, alpha=0.3)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n✓ Forward model analysis complete (sample {sample_idx})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7430721",
   "metadata": {},
   "source": [
    "## Section 5: State-Space Model and LQR Controller Design\n",
    "\n",
    "**Theory:** We model the system as a discrete-time linear system:\n",
    "- **State:** x = [position, velocity]ᵀ\n",
    "- **Dynamics:** x(k+1) = A·x(k) + B·u(k)\n",
    "- **Controller:** u(k) = -K·(x(k) - x_target)\n",
    "\n",
    "The optimal feedback gain K is computed by solving the Discrete Algebraic Riccati Equation (DARE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580652c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define discrete-time state-space matrices\n",
    "# State: [position, velocity]\n",
    "# Dynamics:\n",
    "#   position(k+1) = position(k) + velocity(k)*dt + 0.5*acceleration(k)*dt²\n",
    "#   velocity(k+1) = velocity(k) + acceleration(k)*dt\n",
    "\n",
    "A_lqr = np.array([[1, dt],\n",
    "                  [0, 1]], dtype=float)\n",
    "\n",
    "B_lqr = np.array([[0.5 * dt**2],\n",
    "                  [dt]], dtype=float)\n",
    "\n",
    "print(\"Linear System Matrices:\")\n",
    "print(f\"A (State Transition):\\n{A_lqr}\\n\")\n",
    "print(f\"B (Input Matrix):\\n{B_lqr}\\n\")\n",
    "\n",
    "# Define cost matrices for LQR\n",
    "# Q: penalizes state deviations\n",
    "# R: penalizes control effort\n",
    "Q_lqr = np.array([[100, 0],      # Penalize position error heavily\n",
    "                  [0, 10]], dtype=float)   # Moderate velocity penalty\n",
    "R_lqr = np.array([[1]], dtype=float)       # Small control penalty\n",
    "\n",
    "print(\"Cost Matrices:\")\n",
    "print(f\"Q (State Cost):\\n{Q_lqr}\\n\")\n",
    "print(f\"R (Control Cost):\\n{R_lqr}\\n\")\n",
    "\n",
    "# Solve Discrete Algebraic Riccati Equation (DARE)\n",
    "P_lqr = scipy.linalg.solve_discrete_are(A_lqr, B_lqr, Q_lqr, R_lqr)\n",
    "\n",
    "# Compute optimal feedback gain K\n",
    "K_lqr = np.linalg.inv(R_lqr + B_lqr.T @ P_lqr @ B_lqr) @ (B_lqr.T @ P_lqr @ A_lqr)\n",
    "\n",
    "print(\"DARE Solution:\")\n",
    "print(f\"P (Cost-to-go matrix):\\n{P_lqr}\\n\")\n",
    "print(f\"K (Optimal Feedback Gain):\\n{K_lqr}\\n\")\n",
    "\n",
    "# Display controller interpretation\n",
    "print(\"Control Law Interpretation:\")\n",
    "print(f\"u = -K·(x - x_target)\")\n",
    "print(f\"  = -[{K_lqr[0,0]:.4f}, {K_lqr[0,1]:.4f}] · ([position; velocity] - target)\")\n",
    "print(f\"\\nThis means:\")\n",
    "print(f\"  - Negative position error → {-K_lqr[0,0]:.4f}× feedback (proportional to position)\")\n",
    "print(f\"  - Negative velocity error → {-K_lqr[0,1]:.4f}× feedback (damping)\")\n",
    "\n",
    "print(\"\\n✓ LQR controller designed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8303f0",
   "metadata": {},
   "source": [
    "## Section 6: LQR Controller Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1f9b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate LQR controller to reach target altitude\n",
    "target_height = 10.0\n",
    "sim_duration = 20.0\n",
    "num_steps_sim = int(sim_duration / dt)\n",
    "\n",
    "# Initialize state\n",
    "state_lqr = np.array([[0.0], [0.0]])  # [position, velocity]\n",
    "\n",
    "# Storage for results\n",
    "time_lqr = np.linspace(0, sim_duration, num_steps_sim)\n",
    "pos_lqr = np.zeros(num_steps_sim)\n",
    "vel_lqr = np.zeros(num_steps_sim)\n",
    "acc_lqr = np.zeros(num_steps_sim)\n",
    "control_lqr = np.zeros(num_steps_sim)\n",
    "\n",
    "# Target state\n",
    "target_state = np.array([[target_height], [0.0]])\n",
    "\n",
    "# Simulation loop\n",
    "for i in range(num_steps_sim):\n",
    "    # State error\n",
    "    error_state = state_lqr - target_state\n",
    "    \n",
    "    # Compute control input\n",
    "    u_lqr = -K_lqr @ error_state\n",
    "    control_lqr[i] = u_lqr[0, 0]\n",
    "    \n",
    "    # Store state\n",
    "    pos_lqr[i] = state_lqr[0, 0]\n",
    "    vel_lqr[i] = state_lqr[1, 0]\n",
    "    acc_lqr[i] = u_lqr[0, 0]\n",
    "    \n",
    "    # Update state using system dynamics\n",
    "    state_lqr = A_lqr @ state_lqr + B_lqr @ u_lqr\n",
    "\n",
    "print(f\"LQR Simulation Results:\")\n",
    "print(f\"  Final position: {pos_lqr[-1]:.4f} m (target: {target_height} m)\")\n",
    "print(f\"  Final velocity: {vel_lqr[-1]:.4f} m/s\")\n",
    "print(f\"  Max acceleration: {np.max(np.abs(acc_lqr)):.4f} m/s²\")\n",
    "print(f\"  Max control effort: {np.max(np.abs(control_lqr)):.4f}\")\n",
    "\n",
    "# Plot LQR simulation\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Position\n",
    "axes[0, 0].plot(time_lqr, pos_lqr, linewidth=2.5, color='green', label='Position')\n",
    "axes[0, 0].axhline(y=target_height, color='red', linestyle='--', linewidth=2, label='Target')\n",
    "axes[0, 0].fill_between(time_lqr, pos_lqr, target_height, alpha=0.2, color='green')\n",
    "axes[0, 0].set_ylabel('Position (m)')\n",
    "axes[0, 0].set_title('LQR: Position Trajectory', fontweight='bold')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Velocity\n",
    "axes[0, 1].plot(time_lqr, vel_lqr, linewidth=2.5, color='blue')\n",
    "axes[0, 1].axhline(y=0, color='red', linestyle='--', linewidth=1)\n",
    "axes[0, 1].fill_between(time_lqr, vel_lqr, alpha=0.2, color='blue')\n",
    "axes[0, 1].set_ylabel('Velocity (m/s)')\n",
    "axes[0, 1].set_title('LQR: Velocity Trajectory', fontweight='bold')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Acceleration (Control input)\n",
    "axes[1, 0].plot(time_lqr, acc_lqr, linewidth=2.5, color='purple')\n",
    "axes[1, 0].axhline(y=0, color='red', linestyle='--', linewidth=1)\n",
    "axes[1, 0].fill_between(time_lqr, acc_lqr, alpha=0.2, color='purple')\n",
    "axes[1, 0].set_ylabel('Acceleration (m/s²)')\n",
    "axes[1, 0].set_xlabel('Time (s)')\n",
    "axes[1, 0].set_title('LQR: Control Input (Acceleration)', fontweight='bold')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Phase portrait (velocity vs position)\n",
    "axes[1, 1].plot(pos_lqr, vel_lqr, linewidth=2.5, color='cyan')\n",
    "axes[1, 1].scatter([pos_lqr[0]], [vel_lqr[0]], s=100, color='green', marker='o', \n",
    "                   label='Start', zorder=5)\n",
    "axes[1, 1].scatter([pos_lqr[-1]], [vel_lqr[-1]], s=100, color='red', marker='x', \n",
    "                   label='End', zorder=5)\n",
    "axes[1, 1].scatter([target_height], [0], s=200, color='orange', marker='*', \n",
    "                   label='Target', zorder=5)\n",
    "axes[1, 1].set_xlabel('Position (m)')\n",
    "axes[1, 1].set_ylabel('Velocity (m/s)')\n",
    "axes[1, 1].set_title('LQR: Phase Portrait', fontweight='bold')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ LQR simulation complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c593f64b",
   "metadata": {},
   "source": [
    "## Section 7: LSTM Inverse Model (Controller Learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341b55ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build inverse model: command = f(desired_acceleration_history)\n",
    "# This trains a neural network controller: u = f(historical accelerations)\n",
    "# We swap the roles of input and output\n",
    "\n",
    "inverse_model = Sequential([\n",
    "    LSTM(32, input_shape=(n_timesteps, 1), return_sequences=True, activation='relu'),\n",
    "    LSTM(16, return_sequences=True, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')  # Sigmoid for normalized commands [0,1]\n",
    "])\n",
    "\n",
    "inverse_model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "print(\"Inverse Model (Controller) Architecture:\")\n",
    "inverse_model.summary()\n",
    "\n",
    "# Prepare data: swap X and y roles\n",
    "# Now X_train becomes desired accelerations (targets)\n",
    "# And y_train becomes the commands (inputs we should generate)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Training Inverse LSTM Model (Neural Network Controller)...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "history_inverse = inverse_model.fit(\n",
    "    y_train, X_train,  # Swapped: learn command from desired acceleration\n",
    "    validation_data=(y_test, X_test),\n",
    "    epochs=25,\n",
    "    batch_size=16,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "test_loss_inv = inverse_model.evaluate(y_test, X_test, verbose=0)\n",
    "print(f\"\\nInverse Model Test Loss: {test_loss_inv:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b81a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot inverse model training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 4))\n",
    "\n",
    "axes[0].plot(history_inverse.history['loss'], label='Training Loss', linewidth=2)\n",
    "axes[0].plot(history_inverse.history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss (MSE)')\n",
    "axes[0].set_title('Inverse Model (Controller): Training History', fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_yscale('log')\n",
    "\n",
    "axes[1].plot(history_inverse.history['mae'], label='Training MAE', linewidth=2)\n",
    "axes[1].plot(history_inverse.history['val_mae'], label='Validation MAE', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('MAE')\n",
    "axes[1].set_title('Inverse Model: Mean Absolute Error', fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Inverse model training complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97bd7b6d",
   "metadata": {},
   "source": [
    "## Section 8: Closed-Loop Simulation with Forward & Inverse Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ed31c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate closed-loop control with LSTM forward+inverse models\n",
    "# We'll use the inverse model to compute commands from desired accelerations\n",
    "\n",
    "def simulate_lstm_controller(target_height, sim_duration, forward_model, inverse_model, dt_sim=0.05):\n",
    "    \"\"\"\n",
    "    Simulate closed-loop control using:\n",
    "    1. Inverse model to compute command from desired trajectory\n",
    "    2. Forward model to predict actual acceleration\n",
    "    \"\"\"\n",
    "    num_steps = int(sim_duration / dt_sim)\n",
    "    \n",
    "    # Initialize state\n",
    "    pos = 0.0\n",
    "    vel = 0.0\n",
    "    \n",
    "    # Storage\n",
    "    time_arr = np.linspace(0, sim_duration, num_steps)\n",
    "    pos_hist = np.zeros(num_steps)\n",
    "    vel_hist = np.zeros(num_steps)\n",
    "    acc_hist = np.zeros(num_steps)\n",
    "    cmd_hist = np.zeros(num_steps)\n",
    "    \n",
    "    # Time window for LSTM (use past history)\n",
    "    history_window = min(100, n_timesteps)\n",
    "    past_commands = np.zeros((1, history_window, 1))\n",
    "    \n",
    "    for i in range(num_steps):\n",
    "        # Desired trajectory (ramp to target, then maintain)\n",
    "        time_reach = 10.0  # Seconds to reach target\n",
    "        if i * dt_sim < time_reach:\n",
    "            target_pos = (i * dt_sim / time_reach) * target_height\n",
    "            target_vel = target_height / time_reach\n",
    "        else:\n",
    "            target_pos = target_height\n",
    "            target_vel = 0.0\n",
    "        \n",
    "        # Simple proportional + derivative control for desired acceleration\n",
    "        k_p = 2.0\n",
    "        k_d = 1.0\n",
    "        pos_error = target_pos - pos\n",
    "        vel_error = target_vel - vel\n",
    "        desired_accel = k_p * pos_error + k_d * vel_error\n",
    "        \n",
    "        # Normalize desired acceleration for input to inverse model\n",
    "        desired_accel_norm = (desired_accel / (global_max_abs_y * 2)) * 0.5 + 0.5  # Scale to ~[0,1]\n",
    "        desired_accel_norm = np.clip(desired_accel_norm, 0, 1)\n",
    "        \n",
    "        # Use inverse model to get command\n",
    "        desired_input = np.array([[desired_accel_norm]]).reshape(1, 1, 1)\n",
    "        command = inverse_model.predict(desired_input, verbose=0)[0, 0, 0]\n",
    "        command = np.clip(command, 0, 1)\n",
    "        \n",
    "        # Update history\n",
    "        past_commands = np.roll(past_commands, -1, axis=1)\n",
    "        past_commands[0, -1, 0] = command\n",
    "        \n",
    "        # Use forward model to predict acceleration\n",
    "        predicted_accel_norm = forward_model.predict(past_commands, verbose=0)[0, -1, 0]\n",
    "        actual_accel = predicted_accel_norm * global_max_abs_y\n",
    "        \n",
    "        # Update state with simple integration\n",
    "        vel = vel + actual_accel * dt_sim\n",
    "        pos = pos + vel * dt_sim\n",
    "        \n",
    "        # Store\n",
    "        pos_hist[i] = pos\n",
    "        vel_hist[i] = vel\n",
    "        acc_hist[i] = actual_accel\n",
    "        cmd_hist[i] = command\n",
    "    \n",
    "    return time_arr, pos_hist, vel_hist, acc_hist, cmd_hist\n",
    "\n",
    "# Run LSTM controller simulation\n",
    "target_height_lstm = 10.0\n",
    "sim_duration_lstm = 20.0\n",
    "\n",
    "time_lstm, pos_lstm, vel_lstm, acc_lstm, cmd_lstm = simulate_lstm_controller(\n",
    "    target_height_lstm, sim_duration_lstm, forward_model, inverse_model, dt_sim=0.05\n",
    ")\n",
    "\n",
    "print(f\"LSTM Controller Simulation:\")\n",
    "print(f\"  Final position: {pos_lstm[-1]:.4f} m (target: {target_height_lstm} m)\")\n",
    "print(f\"  Final velocity: {vel_lstm[-1]:.4f} m/s\")\n",
    "print(f\"  Max acceleration: {np.max(np.abs(acc_lstm)):.4f} m/s²\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b61778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot LSTM controller simulation\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Position\n",
    "axes[0, 0].plot(time_lstm, pos_lstm, linewidth=2.5, color='darkgreen', label='LSTM Position')\n",
    "axes[0, 0].plot(time_lqr, pos_lqr, linewidth=2, color='lightgreen', label='LQR Position', alpha=0.7, linestyle='--')\n",
    "axes[0, 0].axhline(y=target_height_lstm, color='red', linestyle='--', linewidth=2, label='Target')\n",
    "axes[0, 0].set_ylabel('Position (m)')\n",
    "axes[0, 0].set_title('Comparison: Position Trajectory', fontweight='bold')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Velocity\n",
    "axes[0, 1].plot(time_lstm, vel_lstm, linewidth=2.5, color='darkblue', label='LSTM Velocity')\n",
    "axes[0, 1].plot(time_lqr, vel_lqr, linewidth=2, color='lightblue', label='LQR Velocity', alpha=0.7, linestyle='--')\n",
    "axes[0, 1].axhline(y=0, color='red', linestyle='--', linewidth=1)\n",
    "axes[0, 1].set_ylabel('Velocity (m/s)')\n",
    "axes[0, 1].set_title('Comparison: Velocity Trajectory', fontweight='bold')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Acceleration / Control\n",
    "axes[1, 0].plot(time_lstm, acc_lstm, linewidth=2.5, color='darkviolet', label='LSTM Acceleration')\n",
    "axes[1, 0].plot(time_lqr, acc_lqr, linewidth=2, color='plum', label='LQR Acceleration', alpha=0.7, linestyle='--')\n",
    "axes[1, 0].axhline(y=0, color='red', linestyle='--', linewidth=1)\n",
    "axes[1, 0].set_ylabel('Acceleration (m/s²)')\n",
    "axes[1, 0].set_xlabel('Time (s)')\n",
    "axes[1, 0].set_title('Comparison: Control Input', fontweight='bold')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Commands\n",
    "axes[1, 1].plot(time_lstm, cmd_lstm, linewidth=2.5, color='darkorange')\n",
    "axes[1, 1].fill_between(time_lstm, cmd_lstm, alpha=0.3, color='darkorange')\n",
    "axes[1, 1].set_xlabel('Time (s)')\n",
    "axes[1, 1].set_ylabel('Normalized Command')\n",
    "axes[1, 1].set_title('LSTM Controller: Command Output', fontweight='bold')\n",
    "axes[1, 1].set_ylim([0, 1])\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ LSTM controller simulation complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6015acb6",
   "metadata": {},
   "source": [
    "## Section 9: Comprehensive Performance Comparison\n",
    "\n",
    "### Comparison Metrics:\n",
    "1. **Tracking Error**: How close actual position is to target\n",
    "2. **Velocity Smoothness**: Variations in velocity (lower is better)\n",
    "3. **Control Effort**: Total energy used (integral of squared control)\n",
    "4. **Rise Time**: Time to reach 90% of target\n",
    "5. **Overshoot**: How much the system exceeds target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee98afeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate performance metrics\n",
    "def compute_metrics(time, position, velocity, control, target_pos, controller_name):\n",
    "    \"\"\"Compute control performance metrics\"\"\"\n",
    "    \n",
    "    # Tracking error\n",
    "    pos_error = np.abs(position - target_pos)\n",
    "    mean_pos_error = np.mean(pos_error)\n",
    "    max_pos_error = np.max(pos_error)\n",
    "    \n",
    "    # Velocity smoothness (derivative)\n",
    "    vel_smoothness = np.mean(np.abs(np.diff(velocity)))\n",
    "    \n",
    "    # Control effort\n",
    "    control_effort = np.sum(control**2) * (time[1] - time[0])\n",
    "    \n",
    "    # Rise time (time to reach 90% of target)\n",
    "    target_90 = 0.9 * target_pos\n",
    "    indices_reached = np.where(position >= target_90)[0]\n",
    "    if len(indices_reached) > 0:\n",
    "        rise_time = time[indices_reached[0]]\n",
    "    else:\n",
    "        rise_time = np.inf\n",
    "    \n",
    "    # Overshoot\n",
    "    max_pos = np.max(position)\n",
    "    overshoot = max(0, max_pos - target_pos)\n",
    "    overshoot_percent = (overshoot / target_pos * 100) if target_pos != 0 else 0\n",
    "    \n",
    "    # Final state error\n",
    "    final_pos_error = np.abs(position[-1] - target_pos)\n",
    "    final_vel_error = np.abs(velocity[-1])\n",
    "    \n",
    "    metrics = {\n",
    "        'Controller': controller_name,\n",
    "        'Mean Pos Error (m)': mean_pos_error,\n",
    "        'Max Pos Error (m)': max_pos_error,\n",
    "        'Final Pos Error (m)': final_pos_error,\n",
    "        'Rise Time (s)': rise_time,\n",
    "        'Overshoot (m)': overshoot,\n",
    "        'Overshoot (%)': overshoot_percent,\n",
    "        'Velocity Smoothness': vel_smoothness,\n",
    "        'Control Effort': control_effort,\n",
    "        'Final Velocity (m/s)': final_vel_error,\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Compute metrics for both controllers\n",
    "metrics_lqr = compute_metrics(time_lqr, pos_lqr, vel_lqr, control_lqr, target_height, 'LQR')\n",
    "metrics_lstm = compute_metrics(time_lstm, pos_lstm, vel_lstm, cmd_lstm, target_height_lstm, 'LSTM')\n",
    "\n",
    "# Display comparison table\n",
    "import pandas as pd\n",
    "metrics_df = pd.DataFrame([metrics_lqr, metrics_lstm])\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"PERFORMANCE COMPARISON: LQR vs LSTM Controller\")\n",
    "print(\"=\"*100)\n",
    "print(metrics_df.to_string(index=False))\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8edc4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed comparison visualization\n",
    "fig = plt.figure(figsize=(16, 12))\n",
    "gs = fig.add_gridspec(3, 3, hspace=0.35, wspace=0.3)\n",
    "\n",
    "# Row 1: Position and errors\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "ax1.plot(time_lqr, pos_lqr, label='LQR', linewidth=2.5, color='green')\n",
    "ax1.plot(time_lstm, pos_lstm, label='LSTM', linewidth=2.5, color='purple', alpha=0.8)\n",
    "ax1.axhline(y=target_height, color='red', linestyle='--', linewidth=2, alpha=0.5, label='Target')\n",
    "ax1.set_ylabel('Position (m)')\n",
    "ax1.set_title('Position Tracking', fontweight='bold')\n",
    "ax1.legend(fontsize=9)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "pos_error_lqr = np.abs(pos_lqr - target_height)\n",
    "pos_error_lstm = np.abs(pos_lstm - target_height_lstm)\n",
    "ax2.semilogy(time_lqr, pos_error_lqr, label='LQR', linewidth=2, color='green')\n",
    "ax2.semilogy(time_lstm, pos_error_lstm, label='LSTM', linewidth=2, color='purple', alpha=0.8)\n",
    "ax2.set_ylabel('Absolute Error (m)')\n",
    "ax2.set_title('Position Tracking Error (log scale)', fontweight='bold')\n",
    "ax2.legend(fontsize=9)\n",
    "ax2.grid(True, alpha=0.3, which='both')\n",
    "\n",
    "ax3 = fig.add_subplot(gs[0, 2])\n",
    "ax3.plot(time_lqr, pos_lqr - target_height, label='LQR', linewidth=2, color='green', alpha=0.7)\n",
    "ax3.plot(time_lstm, pos_lstm - target_height_lstm, label='LSTM', linewidth=2, color='purple', alpha=0.7)\n",
    "ax3.axhline(y=0, color='red', linestyle='--', linewidth=1, alpha=0.5)\n",
    "ax3.fill_between(time_lqr, 0, pos_lqr - target_height, alpha=0.2, color='green')\n",
    "ax3.fill_between(time_lstm, 0, pos_lstm - target_height_lstm, alpha=0.2, color='purple')\n",
    "ax3.set_ylabel('Position Error (m)')\n",
    "ax3.set_title('Position Deviation from Target', fontweight='bold')\n",
    "ax3.legend(fontsize=9)\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Row 2: Velocity and control input\n",
    "ax4 = fig.add_subplot(gs[1, 0])\n",
    "ax4.plot(time_lqr, vel_lqr, label='LQR', linewidth=2.5, color='blue')\n",
    "ax4.plot(time_lstm, vel_lstm, label='LSTM', linewidth=2.5, color='cyan', alpha=0.8)\n",
    "ax4.axhline(y=0, color='red', linestyle='--', linewidth=1, alpha=0.5)\n",
    "ax4.set_ylabel('Velocity (m/s)')\n",
    "ax4.set_title('Velocity Trajectory', fontweight='bold')\n",
    "ax4.legend(fontsize=9)\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "ax5 = fig.add_subplot(gs[1, 1])\n",
    "ax5.plot(time_lqr, acc_lqr, label='LQR', linewidth=2.5, color='darkviolet')\n",
    "ax5.plot(time_lstm, acc_lstm, label='LSTM', linewidth=2.5, color='orange', alpha=0.7)\n",
    "ax5.axhline(y=0, color='red', linestyle='--', linewidth=1, alpha=0.5)\n",
    "ax5.set_ylabel('Acceleration (m/s²)')\n",
    "ax5.set_title('Control Input (Acceleration)', fontweight='bold')\n",
    "ax5.legend(fontsize=9)\n",
    "ax5.grid(True, alpha=0.3)\n",
    "\n",
    "ax6 = fig.add_subplot(gs[1, 2])\n",
    "ax6.plot(time_lqr, np.cumsum(acc_lqr**2) * (time_lqr[1] - time_lqr[0]), \n",
    "         label='LQR', linewidth=2.5, color='darkviolet')\n",
    "ax6.plot(time_lstm, np.cumsum(cmd_lstm**2) * (time_lstm[1] - time_lstm[0]), \n",
    "         label='LSTM', linewidth=2.5, color='orange', alpha=0.7)\n",
    "ax6.set_xlabel('Time (s)')\n",
    "ax6.set_ylabel('Cumulative Energy')\n",
    "ax6.set_title('Control Energy (Cumulative)', fontweight='bold')\n",
    "ax6.legend(fontsize=9)\n",
    "ax6.grid(True, alpha=0.3)\n",
    "\n",
    "# Row 3: Phase portrait and metrics summary\n",
    "ax7 = fig.add_subplot(gs[2, 0])\n",
    "ax7.plot(pos_lqr, vel_lqr, label='LQR', linewidth=2.5, color='green', alpha=0.7)\n",
    "ax7.plot(pos_lstm, vel_lstm, label='LSTM', linewidth=2.5, color='purple', alpha=0.7)\n",
    "ax7.scatter([pos_lqr[0]], [vel_lqr[0]], s=80, marker='o', color='green', zorder=5)\n",
    "ax7.scatter([pos_lstm[0]], [vel_lstm[0]], s=80, marker='o', color='purple', zorder=5)\n",
    "ax7.scatter([pos_lqr[-1]], [vel_lqr[-1]], s=100, marker='x', color='darkgreen', linewidth=3, zorder=5)\n",
    "ax7.scatter([pos_lstm[-1]], [vel_lstm[-1]], s=100, marker='x', color='darkviolet', linewidth=3, zorder=5)\n",
    "ax7.scatter([target_height], [0], s=200, marker='*', color='red', zorder=5, label='Target')\n",
    "ax7.set_xlabel('Position (m)')\n",
    "ax7.set_ylabel('Velocity (m/s)')\n",
    "ax7.set_title('Phase Portrait', fontweight='bold')\n",
    "ax7.legend(fontsize=9)\n",
    "ax7.grid(True, alpha=0.3)\n",
    "\n",
    "# Metrics comparison bar chart\n",
    "ax8 = fig.add_subplot(gs[2, 1:])\n",
    "metrics_names = ['Mean Pos\\nError', 'Rise Time\\n(s)', 'Overshoot\\n(%)', 'Velocity\\nSmoothing', 'Control\\nEffort']\n",
    "lqr_vals = [\n",
    "    metrics_lqr['Mean Pos Error (m)'],\n",
    "    metrics_lqr['Rise Time (s)'] / 10,  # Normalized\n",
    "    metrics_lqr['Overshoot (%)'],\n",
    "    metrics_lqr['Velocity Smoothness'],\n",
    "    metrics_lqr['Control Effort'] / 1000  # Normalized\n",
    "]\n",
    "lstm_vals = [\n",
    "    metrics_lstm['Mean Pos Error (m)'],\n",
    "    metrics_lstm['Rise Time (s)'] / 10,  # Normalized\n",
    "    metrics_lstm['Overshoot (%)'],\n",
    "    metrics_lstm['Velocity Smoothness'],\n",
    "    metrics_lstm['Control Effort'] / 1000  # Normalized\n",
    "]\n",
    "\n",
    "x = np.arange(len(metrics_names))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax8.bar(x - width/2, lqr_vals, width, label='LQR', color='green', alpha=0.7)\n",
    "bars2 = ax8.bar(x + width/2, lstm_vals, width, label='LSTM', color='purple', alpha=0.7)\n",
    "\n",
    "ax8.set_ylabel('Normalized Value')\n",
    "ax8.set_title('Performance Metrics Comparison (Normalized)', fontweight='bold')\n",
    "ax8.set_xticks(x)\n",
    "ax8.set_xticklabels(metrics_names, fontsize=9)\n",
    "ax8.legend()\n",
    "ax8.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        if height > 0:\n",
    "            ax8.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                    f'{height:.2f}', ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Comprehensive comparison complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30dd15d1",
   "metadata": {},
   "source": [
    "## Section 10: Summary and Analysis\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "**LQR Controller:**\n",
    "- Based on linear state-space model\n",
    "- Optimal gain computed analytically via DARE\n",
    "- Fast rise time with minimal overshoot\n",
    "- Smooth control inputs\n",
    "- Robust to linear perturbations\n",
    "- Limited to linear systems\n",
    "\n",
    "**LSTM-Based Controller:**\n",
    "- Data-driven approach learning from experimental data\n",
    "- Can capture nonlinear system dynamics\n",
    "- No need for manual system identification\n",
    "- Can generalize to similar operating conditions\n",
    "- Computational cost: real-time neural network evaluation\n",
    "- May require more data for generalization\n",
    "\n",
    "### Recommendations:\n",
    "1. **Use LQR** when: System is approximately linear, safety is critical, computational resources are limited\n",
    "2. **Use LSTM** when: System is nonlinear, abundant data is available, real-time requirements are moderate\n",
    "3. **Hybrid approach**: Use LSTM for trajectory planning and LQR for stabilization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f00daa",
   "metadata": {},
   "source": [
    "## BONUS Section: System Identification - Estimating A and B from Data\n",
    "\n",
    "**Why identify A and B?**\n",
    "\n",
    "The matrices I used initially were **generic kinematic equations**, assuming:\n",
    "- Perfect integration: `x(k+1) = x(k) + v(k)·dt`\n",
    "- Perfect velocity update: `v(k+1) = v(k) + a(k)·dt`\n",
    "\n",
    "**But your actual system may have:**\n",
    "- Actuator dynamics (delays, nonlinearities)\n",
    "- Friction/damping\n",
    "- Nonlinear thrust response\n",
    "- Inertial effects\n",
    "\n",
    "This section uses **Least Squares System Identification** to extract the true A and B matrices from your experimental data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e7aca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SYSTEM IDENTIFICATION: Extract A and B matrices from experimental data\n",
    "# ============================================================================\n",
    "# \n",
    "# We want to identify a discrete linear system:\n",
    "#   x(k+1) = A·x(k) + B·u(k)\n",
    "#\n",
    "# Where:\n",
    "#   x(k) = [position, velocity]ᵀ\n",
    "#   u(k) = normalized command input\n",
    "#   A = state transition matrix (2x2)\n",
    "#   B = input matrix (2x1)\n",
    "#\n",
    "# Method: Least Squares Identification\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SYSTEM IDENTIFICATION FROM EXPERIMENTAL DATA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Prepare data for system identification\n",
    "# We need to double-integrate acceleration to get position and velocity\n",
    "\n",
    "sample_idx = 0  # Use first sample for identification\n",
    "command_sequence = X_train[sample_idx, :, 0]  # Normalized commands\n",
    "acceleration_sequence = y_train[sample_idx, :, 0] * global_max_abs_y  # Denormalized acceleration\n",
    "\n",
    "# Integrate to get velocity and position\n",
    "velocity_sequence = np.cumsum(acceleration_sequence) * dt\n",
    "position_sequence = np.cumsum(velocity_sequence) * dt\n",
    "\n",
    "print(f\"\\nUsing sample {sample_idx} for identification:\")\n",
    "print(f\"  Command length: {len(command_sequence)}\")\n",
    "print(f\"  Acceleration: min={np.min(acceleration_sequence):.4f}, max={np.max(acceleration_sequence):.4f} m/s²\")\n",
    "print(f\"  Velocity: min={np.min(velocity_sequence):.4f}, max={np.max(velocity_sequence):.4f} m/s\")\n",
    "print(f\"  Position: min={np.min(position_sequence):.4f}, max={np.max(position_sequence):.4f} m\")\n",
    "\n",
    "# Build regression matrices for least squares\n",
    "# Stack multiple time instances to get overdetermined system\n",
    "n_data_points = len(command_sequence) - 1\n",
    "\n",
    "# State at time k: [position(k), velocity(k)]ᵀ\n",
    "X_k = np.column_stack([position_sequence[:-1], velocity_sequence[:-1]])  # (n_data_points, 2)\n",
    "\n",
    "# State at time k+1\n",
    "X_kp1 = np.column_stack([position_sequence[1:], velocity_sequence[1:]])  # (n_data_points, 2)\n",
    "\n",
    "# Input at time k\n",
    "U_k = command_sequence[:-1].reshape(-1, 1)  # (n_data_points, 1)\n",
    "\n",
    "# Build augmented matrix: [X(k) | U(k)]\n",
    "Phi = np.column_stack([X_k, U_k])  # (n_data_points, 3)\n",
    "\n",
    "# Solve least squares: X(k+1) = Phi * theta\n",
    "# where theta = [A11, A12, B1; A21, A22, B2]ᵀ\n",
    "\n",
    "theta, residuals, rank, s = np.linalg.lstsq(Phi, X_kp1, rcond=None)\n",
    "\n",
    "# Extract identified matrices\n",
    "A_identified = theta[:2, :2].T  # Shape: (2, 2) - Note: theta is (3, 2)\n",
    "B_identified = theta[2:3, :].T   # Shape: (2, 1)\n",
    "\n",
    "# Properly reshape theta for 2x2 A and 2x1 B\n",
    "# theta should be interpreted as columns of the result\n",
    "A_identified = theta[:, :2].T  # First 2 rows (for each state equation)\n",
    "B_identified = theta[:, 2:3].T  # Last row\n",
    "\n",
    "# Actually, let's be more careful:\n",
    "# For each state component, we have: x_i(k+1) = A_i · [x1(k), x2(k)] + B_i · u(k)\n",
    "# This is 2 equations (for 2 state components)\n",
    "\n",
    "A_identified = theta[:2, :].T  # theta has shape (3, 2), take first 2 rows, transpose\n",
    "B_identified = theta[2, :].reshape(-1, 1)  # Take last row\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"IDENTIFIED STATE-SPACE MATRICES (from Least Squares)\")\n",
    "print(\"-\"*80)\n",
    "print(f\"\\nA_identified (State Transition):\\n{A_identified}\")\n",
    "print(f\"\\nB_identified (Input Matrix):\\n{B_identified}\")\n",
    "\n",
    "# For comparison, print the theoretical matrices\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"THEORETICAL KINEMATIC MATRICES (for reference)\")\n",
    "print(\"-\"*80)\n",
    "print(f\"\\nA_theoretical:\\n{A_lqr}\")\n",
    "print(f\"\\nB_theoretical:\\n{B_lqr}\")\n",
    "\n",
    "# Calculate identification error\n",
    "residual_error = np.mean(residuals) if len(residuals) > 0 else 0\n",
    "print(f\"\\nIdentification Residual Error: {residual_error:.6e}\")\n",
    "\n",
    "# Validate identification by predicting future states\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"VALIDATION: Compare Identified Model vs Actual Data\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Simulate with identified model\n",
    "x_sim_identified = np.zeros((len(command_sequence), 2))\n",
    "x_sim_identified[0] = [position_sequence[0], velocity_sequence[0]]\n",
    "\n",
    "x_actual = np.column_stack([position_sequence, velocity_sequence])\n",
    "\n",
    "for k in range(len(command_sequence) - 1):\n",
    "    x_sim_identified[k+1] = A_identified @ x_sim_identified[k] + B_identified.flatten() * U_k[k, 0]\n",
    "\n",
    "# Calculate prediction errors\n",
    "pos_pred_error = np.mean(np.abs(x_sim_identified[:, 0] - position_sequence))\n",
    "vel_pred_error = np.mean(np.abs(x_sim_identified[:, 1] - velocity_sequence))\n",
    "\n",
    "print(f\"\\nMean Absolute Prediction Error:\")\n",
    "print(f\"  Position: {pos_pred_error:.6f} m\")\n",
    "print(f\"  Velocity: {vel_pred_error:.6f} m/s\")\n",
    "\n",
    "# Visualization comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Acceleration data\n",
    "axes[0, 0].plot(acceleration_sequence, label='Measured', linewidth=2, color='blue')\n",
    "axes[0, 0].fill_between(range(len(acceleration_sequence)), acceleration_sequence, alpha=0.3, color='blue')\n",
    "axes[0, 0].set_ylabel('Acceleration (m/s²)')\n",
    "axes[0, 0].set_title('Raw Acceleration Data', fontweight='bold')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# Integrated velocity\n",
    "axes[0, 1].plot(velocity_sequence, label='Measured', linewidth=2, color='green')\n",
    "axes[0, 1].plot(x_sim_identified[:, 1], label='Identified Model', linewidth=2, \n",
    "               color='red', alpha=0.7, linestyle='--')\n",
    "axes[0, 1].set_ylabel('Velocity (m/s)')\n",
    "axes[0, 1].set_title('Velocity: Measured vs Identified Model', fontweight='bold')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# Integrated position\n",
    "axes[1, 0].plot(position_sequence, label='Measured', linewidth=2, color='purple')\n",
    "axes[1, 0].plot(x_sim_identified[:, 0], label='Identified Model', linewidth=2, \n",
    "               color='orange', alpha=0.7, linestyle='--')\n",
    "axes[1, 0].set_ylabel('Position (m)')\n",
    "axes[1, 0].set_title('Position: Measured vs Identified Model', fontweight='bold')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# Phase portrait\n",
    "axes[1, 1].plot(position_sequence, velocity_sequence, label='Measured', linewidth=2, color='green')\n",
    "axes[1, 1].plot(x_sim_identified[:, 0], x_sim_identified[:, 1], label='Identified Model', \n",
    "               linewidth=2, color='red', alpha=0.7, linestyle='--')\n",
    "axes[1, 1].scatter([position_sequence[0]], [velocity_sequence[0]], s=100, marker='o', \n",
    "                  color='green', zorder=5, label='Start')\n",
    "axes[1, 1].scatter([position_sequence[-1]], [velocity_sequence[-1]], s=100, marker='x', \n",
    "                  color='green', zorder=5)\n",
    "axes[1, 1].set_xlabel('Position (m)')\n",
    "axes[1, 1].set_ylabel('Velocity (m/s)')\n",
    "axes[1, 1].set_title('Phase Portrait: Measured vs Identified Model', fontweight='bold')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "axes[1, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ System identification complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7938bfcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# LQR CONTROLLER WITH IDENTIFIED MATRICES\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LQR CONTROLLER DESIGN WITH IDENTIFIED SYSTEM\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Solve DARE with identified matrices\n",
    "try:\n",
    "    P_identified = scipy.linalg.solve_discrete_are(A_identified, B_identified, Q_lqr, R_lqr)\n",
    "    K_identified = np.linalg.inv(R_lqr + B_identified.T @ P_identified @ B_identified) @ \\\n",
    "                   (B_identified.T @ P_identified @ A_identified)\n",
    "    \n",
    "    print(\"\\nIdentified System LQR Gain:\")\n",
    "    print(f\"K_identified:\\n{K_identified}\")\n",
    "    print(f\"\\nComparison with Theoretical Gain:\")\n",
    "    print(f\"K_theoretical:\\n{K_lqr}\")\n",
    "    print(f\"\\nGain Difference: {np.linalg.norm(K_identified - K_lqr):.6f}\")\n",
    "    \n",
    "    # Simulate with identified matrices\n",
    "    state_identified = np.array([[0.0], [0.0]])\n",
    "    target_state = np.array([[target_height], [0.0]])\n",
    "    \n",
    "    pos_identified = np.zeros(num_steps_sim)\n",
    "    vel_identified = np.zeros(num_steps_sim)\n",
    "    acc_identified = np.zeros(num_steps_sim)\n",
    "    \n",
    "    for i in range(num_steps_sim):\n",
    "        error_state = state_identified - target_state\n",
    "        u_identified = -K_identified @ error_state\n",
    "        acc_identified[i] = u_identified[0, 0]\n",
    "        \n",
    "        pos_identified[i] = state_identified[0, 0]\n",
    "        vel_identified[i] = state_identified[1, 0]\n",
    "        \n",
    "        state_identified = A_identified @ state_identified + B_identified @ u_identified\n",
    "    \n",
    "    print(f\"\\nIdentified System LQR Performance:\")\n",
    "    print(f\"  Final position: {pos_identified[-1]:.4f} m (target: {target_height} m)\")\n",
    "    print(f\"  Final velocity: {vel_identified[-1]:.4f} m/s\")\n",
    "    print(f\"  Max acceleration: {np.max(np.abs(acc_identified)):.4f} m/s²\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not compute LQR for identified system: {e}\")\n",
    "    print(\"This may occur if the system is not controllable.\")\n",
    "    pos_identified = pos_lqr.copy()\n",
    "    vel_identified = vel_lqr.copy()\n",
    "    acc_identified = acc_lqr.copy()\n",
    "\n",
    "# Comparison plot\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Position comparison\n",
    "axes[0, 0].plot(time_lqr, pos_lqr, label='LQR (Theoretical)', linewidth=2.5, color='green', alpha=0.7)\n",
    "axes[0, 0].plot(time_lqr, pos_identified, label='LQR (Identified)', linewidth=2.5, \n",
    "               color='darkgreen', alpha=0.7, linestyle='--')\n",
    "axes[0, 0].axhline(y=target_height, color='red', linestyle='--', linewidth=2, alpha=0.5, label='Target')\n",
    "axes[0, 0].set_ylabel('Position (m)')\n",
    "axes[0, 0].set_title('Position: Theoretical vs Identified Matrices', fontweight='bold')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Velocity comparison\n",
    "axes[0, 1].plot(time_lqr, vel_lqr, label='LQR (Theoretical)', linewidth=2.5, color='blue', alpha=0.7)\n",
    "axes[0, 1].plot(time_lqr, vel_identified, label='LQR (Identified)', linewidth=2.5, \n",
    "               color='darkblue', alpha=0.7, linestyle='--')\n",
    "axes[0, 1].axhline(y=0, color='red', linestyle='--', linewidth=1, alpha=0.5)\n",
    "axes[0, 1].set_ylabel('Velocity (m/s)')\n",
    "axes[0, 1].set_title('Velocity: Theoretical vs Identified Matrices', fontweight='bold')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Control input comparison\n",
    "axes[1, 0].plot(time_lqr, acc_lqr, label='LQR (Theoretical)', linewidth=2.5, color='purple', alpha=0.7)\n",
    "axes[1, 0].plot(time_lqr, acc_identified, label='LQR (Identified)', linewidth=2.5, \n",
    "               color='darkviolet', alpha=0.7, linestyle='--')\n",
    "axes[1, 0].axhline(y=0, color='red', linestyle='--', linewidth=1, alpha=0.5)\n",
    "axes[1, 0].set_xlabel('Time (s)')\n",
    "axes[1, 0].set_ylabel('Acceleration (m/s²)')\n",
    "axes[1, 0].set_title('Control Input: Theoretical vs Identified', fontweight='bold')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Matrix comparison visualization\n",
    "ax4 = axes[1, 1]\n",
    "ax4.axis('off')\n",
    "\n",
    "matrix_text = f\"\"\"\n",
    "SYSTEM MATRICES COMPARISON\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "\n",
    "Theoretical Matrices:\n",
    "A_theoretical = \n",
    "  [{A_lqr[0,0]:.6f}  {A_lqr[0,1]:.6f}]\n",
    "  [{A_lqr[1,0]:.6f}  {A_lqr[1,1]:.6f}]\n",
    "\n",
    "B_theoretical = \n",
    "  [{B_lqr[0,0]:.6f}]\n",
    "  [{B_lqr[1,0]:.6f}]\n",
    "\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "\n",
    "Identified Matrices:\n",
    "A_identified = \n",
    "  [{A_identified[0,0]:.6f}  {A_identified[0,1]:.6f}]\n",
    "  [{A_identified[1,0]:.6f}  {A_identified[1,1]:.6f}]\n",
    "\n",
    "B_identified = \n",
    "  [{B_identified[0,0]:.6f}]\n",
    "  [{B_identified[1,0]:.6f}]\n",
    "\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "\n",
    "System Quality Indicator:\n",
    "✓ If differences are small: \n",
    "  Theoretical model matches system\n",
    "\n",
    "✗ If differences are large:\n",
    "  System has additional dynamics\n",
    "  (damping, friction, nonlinearity)\n",
    "\"\"\"\n",
    "\n",
    "ax4.text(0.05, 0.95, matrix_text, transform=ax4.transAxes, fontfamily='monospace',\n",
    "         verticalalignment='top', fontsize=9, bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Comparison complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf307e7",
   "metadata": {},
   "source": [
    "## Final Summary: Why Matrices Matter\n",
    "\n",
    "### Three Approaches to Get A and B:\n",
    "\n",
    "**1. Theoretical/Kinematic (What I initially used)** ❌ Arbitrary\n",
    "```\n",
    "A = [1    dt  ]    B = [0.5*dt²]\n",
    "    [0    1   ]        [dt     ]\n",
    "```\n",
    "- ✅ Simple, interpretable\n",
    "- ❌ Only works if system perfectly follows kinematics\n",
    "- ❌ Ignores actuator dynamics, friction, delays\n",
    "\n",
    "**2. System Identification (BEST) ✅ Data-driven**\n",
    "```\n",
    "From your experimental data, estimate:\n",
    "  min ||X(k+1) - [A·X(k) + B·U(k)]||²\n",
    "```\n",
    "- ✅ Captures real system behavior\n",
    "- ✅ Accounts for actuator nonlinearities\n",
    "- ✅ Minimal assumptions\n",
    "- ✓ Validated against your actual data\n",
    "\n",
    "**3. Physics-Based Modeling** 🔬 (If available)\n",
    "- Measure inertia, friction coefficients\n",
    "- Determine actuator response curves\n",
    "- Model aerodynamic effects\n",
    "\n",
    "### What Your Identification Shows:\n",
    "\n",
    "If `A_identified ≈ A_theoretical`:\n",
    "- ✅ Your system behaves linearly\n",
    "- ✅ Kinematic assumptions are valid\n",
    "- ✅ LQR with theoretical matrices should work well\n",
    "\n",
    "If `A_identified ≠ A_theoretical`:\n",
    "- ⚠️ System has nonlinear dynamics\n",
    "- ⚠️ May need higher-order model\n",
    "- ⚠️ LSTM approach becomes more attractive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552dfd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary visualization\n",
    "fig = plt.figure(figsize=(16, 10))\n",
    "gs = fig.add_gridspec(2, 3, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# Controller architecture comparison\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "ax1.axis('off')\n",
    "lqr_text = \"\"\"\n",
    "LQR (Linear Quadratic Regulator)\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "Approach: Classical Control\n",
    "Model: Discrete Linear State-Space\n",
    "Optimization: DARE (Riccati Equation)\n",
    "Computation: Closed-form solution\n",
    "Online Cost: Matrix multiplication O(n²)\n",
    "\n",
    "Architecture:\n",
    "  System Dynamics: Linear\n",
    "  Control: u = -K(x - x_target)\n",
    "  State: [position, velocity]\n",
    "  \n",
    "Advantages:\n",
    "  ✓ Guaranteed stability\n",
    "  ✓ Optimal for linear systems\n",
    "  ✓ Low computational cost\n",
    "  ✓ Interpretable\n",
    "  \n",
    "Disadvantages:\n",
    "  ✗ Limited to linear systems\n",
    "  ✗ Requires system identification\n",
    "  ✗ May not capture nonlinearities\n",
    "\"\"\"\n",
    "ax1.text(0.05, 0.95, lqr_text, transform=ax1.transAxes, fontfamily='monospace',\n",
    "         verticalalignment='top', fontsize=9, bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.3))\n",
    "\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "ax2.axis('off')\n",
    "lstm_text = \"\"\"\n",
    "LSTM Neural Network Controller\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "Approach: Data-Driven (ML)\n",
    "Model: LSTM + Dense Layers\n",
    "Optimization: Backpropagation\n",
    "Computation: Learned from data\n",
    "Online Cost: Neural net inference\n",
    "\n",
    "Architecture:\n",
    "  Forward Model: Command → Accel\n",
    "  Inverse Model: Accel → Command\n",
    "  Control: Learned by LSTM network\n",
    "  State: Historical sequences\n",
    "  \n",
    "Advantages:\n",
    "  ✓ Captures nonlinearities\n",
    "  ✓ No manual identification needed\n",
    "  ✓ Learns from data patterns\n",
    "  ✓ Can generalize\n",
    "  \n",
    "Disadvantages:\n",
    "  ✗ Requires training data\n",
    "  ✗ Computational overhead\n",
    "  ✗ Less interpretable\n",
    "  ✗ May overfit\n",
    "\"\"\"\n",
    "ax2.text(0.05, 0.95, lstm_text, transform=ax2.transAxes, fontfamily='monospace',\n",
    "         verticalalignment='top', fontsize=9, bbox=dict(boxstyle='round', facecolor='mediumpurple', alpha=0.3))\n",
    "\n",
    "ax3 = fig.add_subplot(gs[0, 2])\n",
    "ax3.axis('off')\n",
    "comparison_text = f\"\"\"\n",
    "Quantitative Comparison\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "\n",
    "Metric                  LQR        LSTM\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "Rise Time           {metrics_lqr['Rise Time (s)']:6.2f}s   {metrics_lstm['Rise Time (s)']:6.2f}s\n",
    "Mean Pos Error      {metrics_lqr['Mean Pos Error (m)']:6.4f}m   {metrics_lstm['Mean Pos Error (m)']:6.4f}m\n",
    "Max Overshoot       {metrics_lqr['Overshoot (%)']:6.2f}%   {metrics_lstm['Overshoot (%)']:6.2f}%\n",
    "Final Pos Error     {metrics_lqr['Final Pos Error (m)']:6.4f}m   {metrics_lstm['Final Pos Error (m)']:6.4f}m\n",
    "Velocity Smooth     {metrics_lqr['Velocity Smoothness']:6.4f}   {metrics_lstm['Velocity Smoothness']:6.4f}\n",
    "Control Energy      {metrics_lqr['Control Effort']:6.2f}   {metrics_lstm['Control Effort']:6.2f}\n",
    "\n",
    "Key Observation:\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "LQR provides optimal linear control\n",
    "with guaranteed convergence.\n",
    "\n",
    "LSTM learns data patterns but may\n",
    "require tuning for best performance.\n",
    "\"\"\"\n",
    "ax3.text(0.05, 0.95, comparison_text, transform=ax3.transAxes, fontfamily='monospace',\n",
    "         verticalalignment='top', fontsize=8.5, bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.5))\n",
    "\n",
    "# Bottom row: Decision tree\n",
    "ax4 = fig.add_subplot(gs[1, :])\n",
    "ax4.axis('off')\n",
    "\n",
    "decision_text = \"\"\"\n",
    "DECISION FRAMEWORK: Which Controller to Use?\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "\n",
    "                                        ┌─── System Well-Characterized? ───┐\n",
    "                                        │                                  │\n",
    "                                     YES                               NO (or High uncertainty)\n",
    "                                        │                                  │\n",
    "                        ┌───────────────┴──────────┐            ┌─────────┴──────────────┐\n",
    "                        │                          │            │                        │\n",
    "                    Linear System?           Nonlinear System?   Enough Data Available?  │\n",
    "                        │                          │            │                        │\n",
    "                    ╔═══╩═══╗                  ╔═══╩═══╗        │                        │\n",
    "                    ║ LQR ✓ ║                  ║ LSTM  ║    YES │                    NO │\n",
    "                    ║    ✓  ║ OPTIMAL          ║ Hybrid║        │                        │\n",
    "                    ╚═══════╝                  ╚═══════╝        │                        │\n",
    "                    Fast, Proven                 Good if        │                        │\n",
    "                    Guaranteed Stability         data/compute   │                        │\n",
    "                                                available      ╔══════════════════╗  ╔═════════════════╗\n",
    "                    Safety Critical?                           ║ LQR or PID      ║  ║ Collect Data or ║\n",
    "                         ║                                     ║ + Learning      ║  ║ Model Externally║\n",
    "                    ╔════╩════╗                                ╚══════════════════╝  ╚═════════════════╝\n",
    "                    ║  LQR    ║\n",
    "                    ║ PREFERRED║\n",
    "                    ╚═════════╝\n",
    "                    Predictable, \n",
    "                    Analyzable\n",
    "\n",
    "HYBRID STRATEGY: LSTM for Planning + LQR for Stabilization\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "\n",
    "  Reference Trajectory                  LSTM Trajectory             LQR Stabilization          Final Trajectory\n",
    "  (Target Path)          ───────→       Generator (Plan)   ─────→   (Stabilize around)  ─────→  (Smooth, Stable)\n",
    "                                        \n",
    "                         Data-Driven    Provides rough plan          Classical control    Guaranteed convergence\n",
    "                         approach       Good for complex paths       Linear corrections   + Adaptability\n",
    "\"\"\"\n",
    "\n",
    "ax4.text(0.02, 0.98, decision_text, transform=ax4.transAxes, fontfamily='monospace',\n",
    "         verticalalignment='top', fontsize=8.5, bbox=dict(boxstyle='round', facecolor='lightcyan', alpha=0.4))\n",
    "\n",
    "plt.suptitle('State-Space Control vs LSTM-Based Control: Complete Analysis', \n",
    "             fontsize=14, fontweight='bold', y=0.98)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"ANALYSIS COMPLETE\")\n",
    "print(\"=\"*100)\n",
    "print(f\"\\nNotebook generated: {len([1 for _ in [1,2,3,4,5,6,7,8,9,10]])} sections with comprehensive plots and analysis\")\n",
    "print(\"\\nKey Outputs:\")\n",
    "print(\"  1. Forward LSTM Model: System dynamics learning\")\n",
    "print(\"  2. LQR Controller: Analytical optimal control\")\n",
    "print(\"  3. LSTM Controller: Data-driven control\")\n",
    "print(\"  4. Performance Metrics: Quantitative comparison\")\n",
    "print(\"  5. Visualization: 15+ comparison plots\")\n",
    "print(\"  6. Decision Framework: Guidance for controller selection\")\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659e11d2",
   "metadata": {},
   "source": [
    "# ============================================================================\n",
    "## Section 11: State Space Models (SSM) for Sequence Modeling\n",
    "## ============================================================================\n",
    "\n",
    "### Theory: SSM Introduction\n",
    "\n",
    "State Space Models (SSMs) are a powerful framework for modeling **sequential and temporal data**. Originally used in control theory, SSMs have been recently rediscovered for deep learning (S4, Mamba, etc.).\n",
    "\n",
    "**Key Insight:** SSMs represent data as continuous dynamical systems that are discretized for computation.\n",
    "\n",
    "### Continuous SSM Formulation\n",
    "\n",
    "A continuous SSM is defined by four learnable matrices and two equations:\n",
    "\n",
    "$$\\boxed{\\begin{align}\n",
    "x'(t) &= Ax(t) + Bu(t) \\quad \\text{(State equation)} \\\\\n",
    "y(t) &= Cx(t) + Du(t) \\quad \\text{(Output equation)}\n",
    "\\end{align}}$$\n",
    "\n",
    "**Where:**\n",
    "- $x(t) \\in \\mathbb{R}^n$ : Hidden state (dimension $n$)\n",
    "- $u(t) \\in \\mathbb{R}^m$ : Input signal (dimension $m$)\n",
    "- $y(t) \\in \\mathbb{R}^p$ : Output signal (dimension $p$)\n",
    "- $A \\in \\mathbb{R}^{n \\times n}$ : State transition matrix\n",
    "- $B \\in \\mathbb{R}^{n \\times m}$ : Input matrix\n",
    "- $C \\in \\mathbb{R}^{p \\times n}$ : Output matrix\n",
    "- $D \\in \\mathbb{R}^{p \\times m}$ : Feedthrough matrix (often set to 0)\n",
    "\n",
    "**Simplified form** (removing $D$ as a skip connection):\n",
    "$$\\boxed{\\begin{align}\n",
    "x'(t) &= Ax(t) + Bu(t) \\\\\n",
    "y(t) &= Cx(t)\n",
    "\\end{align}}$$\n",
    "\n",
    "### Three Views of SSMs\n",
    "\n",
    "SSMs have **three equivalent representations**:\n",
    "\n",
    "1. **Continuous View** ✓ Handles continuous/irregular data\n",
    "2. **Recursive View** ✓ Efficient inference (like RNNs)\n",
    "3. **Convolutional View** ✓ Efficient training (parallelizable)\n",
    "\n",
    "### Discretization: From Continuous to Discrete\n",
    "\n",
    "To implement SSMs on computers, we must discretize the continuous equations. Using the **trapezoid/bilinear method**:\n",
    "\n",
    "$$\\Delta = t_{n+1} - t_n \\quad \\text{(sampling interval)}$$\n",
    "\n",
    "$$x_n' = Ax_n + Bu_n$$\n",
    "\n",
    "Apply trapezoid rule:\n",
    "$$x_{n+1} - x_n = \\frac{\\Delta}{2}(Ax_n + Bu_n + Ax_{n+1} + Bu_{n+1})$$\n",
    "\n",
    "Rearrange:\n",
    "$$\\boxed{x_{n+1} = \\bar{A}x_n + \\bar{B}u_n}$$\n",
    "\n",
    "$$\\boxed{y_n = \\bar{C}x_n}$$\n",
    "\n",
    "**Where the discretized matrices are:**\n",
    "\n",
    "$$\\boxed{\\begin{align}\n",
    "\\bar{A} &= (I - \\frac{\\Delta}{2}A)^{-1}(I + \\frac{\\Delta}{2}A) \\\\\n",
    "\\bar{B} &= (I - \\frac{\\Delta}{2}A)^{-1} \\Delta B \\\\\n",
    "\\bar{C} &= C\n",
    "\\end{align}}$$\n",
    "\n",
    "### Recursive (RNN-like) View\n",
    "\n",
    "Process sequences step-by-step:\n",
    "\n",
    "$$\\boxed{\\begin{align}\n",
    "x_k &= \\bar{A}x_{k-1} + \\bar{B}u_k \\quad \\text{(Recurrence)} \\\\\n",
    "y_k &= \\bar{C}x_k \\quad \\text{(Output)}\n",
    "\\end{align}}$$\n",
    "\n",
    "Advantages:\n",
    "- ✓ Efficient inference (constant-time updates)\n",
    "- ✗ Slow training (not parallelizable)\n",
    "\n",
    "### Convolutional (CNN-like) View\n",
    "\n",
    "By unrolling the recurrence, we can express output as a convolution:\n",
    "\n",
    "$$y_k = \\bar{C}(\\bar{A}^k\\bar{B}u_0 + \\bar{A}^{k-1}\\bar{B}u_1 + \\cdots + \\bar{B}u_k)$$\n",
    "\n",
    "$$\\boxed{y_k = \\sum_{j=0}^{k} \\bar{K}_j u_{k-j}}$$\n",
    "\n",
    "**Where $\\bar{K}$ is the convolutional kernel:**\n",
    "\n",
    "$$\\boxed{\\bar{K}_k = \\bar{C}\\bar{A}^k\\bar{B}}$$\n",
    "\n",
    "Advantages:\n",
    "- ✓ Efficient training (parallelizable via FFT)\n",
    "- ✗ Slow inference (must recalculate entire sequence)\n",
    "\n",
    "### Comparison: Recursive vs Convolutional\n",
    "\n",
    "| Aspect | Recursive | Convolutional |\n",
    "|--------|-----------|--------------|\n",
    "| **Training Speed** | Slow (sequential) | Fast (parallel) |\n",
    "| **Inference Speed** | Fast (constant time) | Slow (full sequence) |\n",
    "| **Memory** | $O(1)$ | $O(L)$ where $L$ = sequence length |\n",
    "| **Best For** | Online/streaming | Batch processing |\n",
    "\n",
    "**Practical Strategy:**\n",
    "- **Train** using convolutional view (fast)\n",
    "- **Infer** using recursive view (efficient)\n",
    "\n",
    "### Matrix Initialization: HiPPO\n",
    "\n",
    "Random initialization of $A$ leads to poor results. The **HiPPO (High-Order Polynomial Projection Operator)** matrix provides a principled initialization optimized for temporal dynamics.\n",
    "\n",
    "Key properties:\n",
    "- Captures Legendre polynomial dynamics\n",
    "- Enables $O(1)$ memory for long sequences\n",
    "- Diagonal or NPLR structure for efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd837bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SSM IMPLEMENTATION FOR CONTROL SEQUENCES\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STATE SPACE MODEL (SSM) IMPLEMENTATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "class SSMModel:\n",
    "    \"\"\"\n",
    "    State Space Model for sequence modeling\n",
    "    \n",
    "    Continuous form:\n",
    "        x'(t) = A·x(t) + B·u(t)\n",
    "        y(t) = C·x(t)\n",
    "    \n",
    "    Discrete form (after trapezoid discretization):\n",
    "        x_k = Ā·x_{k-1} + B̄·u_k\n",
    "        y_k = C̄·x_k\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, input_dim, output_dim, dt=0.05, init_method='random'):\n",
    "        \"\"\"\n",
    "        Initialize SSM with given dimensions\n",
    "        \n",
    "        Args:\n",
    "            state_dim: Dimension of hidden state (n)\n",
    "            input_dim: Dimension of input (m)\n",
    "            output_dim: Dimension of output (p)\n",
    "            dt: Discretization interval (Δ)\n",
    "            init_method: 'random', 'hippopotamus' (HiPPO), or 'stable'\n",
    "        \"\"\"\n",
    "        self.n = state_dim      # Hidden state dimension\n",
    "        self.m = input_dim      # Input dimension\n",
    "        self.p = output_dim     # Output dimension\n",
    "        self.dt = dt            # Sampling time\n",
    "        \n",
    "        # Initialize continuous matrices\n",
    "        if init_method == 'random':\n",
    "            self.A = np.random.randn(self.n, self.n) * 0.1\n",
    "        elif init_method == 'stable':\n",
    "            # Ensure stable system (all eigenvalues have negative real part)\n",
    "            A_rand = np.random.randn(self.n, self.n)\n",
    "            self.A = A_rand - 2 * np.eye(self.n)  # Shift to negative region\n",
    "        elif init_method == 'diagonal':\n",
    "            # Diagonal matrix with negative eigenvalues\n",
    "            self.A = np.diag(-np.exp(np.random.uniform(0, 2, self.n)))\n",
    "        else:\n",
    "            self.A = np.random.randn(self.n, self.n) * 0.1\n",
    "        \n",
    "        self.B = np.random.randn(self.n, self.m) * 0.1\n",
    "        self.C = np.random.randn(self.p, self.n) * 0.1\n",
    "        \n",
    "        # Discretize using trapezoid method\n",
    "        self._discretize()\n",
    "        \n",
    "    def _discretize(self):\n",
    "        \"\"\"\n",
    "        Discretize continuous SSM using trapezoid (bilinear) method\n",
    "        \n",
    "        Ā = (I - Δ/2·A)^(-1) · (I + Δ/2·A)\n",
    "        B̄ = (I - Δ/2·A)^(-1) · Δ·B\n",
    "        C̄ = C\n",
    "        \"\"\"\n",
    "        I = np.eye(self.n)\n",
    "        \n",
    "        # Factor: (I - Δ/2·A)\n",
    "        left_factor = I - (self.dt / 2) * self.A\n",
    "        right_factor = I + (self.dt / 2) * self.A\n",
    "        \n",
    "        # Compute A_bar with regularization for numerical stability\n",
    "        try:\n",
    "            left_inv = np.linalg.inv(left_factor)\n",
    "            self.A_bar = left_inv @ right_factor\n",
    "            self.B_bar = left_inv @ (self.dt * self.B)\n",
    "            self.C_bar = self.C.copy()\n",
    "        except np.linalg.LinAlgError:\n",
    "            print(\"Warning: Singular matrix in discretization, using regularization\")\n",
    "            left_factor += 1e-6 * np.eye(self.n)\n",
    "            left_inv = np.linalg.inv(left_factor)\n",
    "            self.A_bar = left_inv @ right_factor\n",
    "            self.B_bar = left_inv @ (self.dt * self.B)\n",
    "            self.C_bar = self.C.copy()\n",
    "    \n",
    "    def forward_recursive(self, u_sequence):\n",
    "        \"\"\"\n",
    "        Recursive (RNN-like) forward pass\n",
    "        \n",
    "        x_k = Ā·x_{k-1} + B̄·u_k\n",
    "        y_k = C̄·x_k\n",
    "        \n",
    "        Args:\n",
    "            u_sequence: (L, m) array of input sequence\n",
    "        \n",
    "        Returns:\n",
    "            y_sequence: (L, p) array of output sequence\n",
    "            x_states: (L+1, n) array of hidden states\n",
    "        \"\"\"\n",
    "        L = u_sequence.shape[0]\n",
    "        y_sequence = np.zeros((L, self.p))\n",
    "        x_states = np.zeros((L + 1, self.n))\n",
    "        \n",
    "        for k in range(L):\n",
    "            # Update state\n",
    "            x_states[k + 1] = self.A_bar @ x_states[k] + self.B_bar @ u_sequence[k]\n",
    "            # Compute output\n",
    "            y_sequence[k] = self.C_bar @ x_states[k + 1]\n",
    "        \n",
    "        return y_sequence, x_states\n",
    "    \n",
    "    def get_convolution_kernel(self, length):\n",
    "        \"\"\"\n",
    "        Compute convolutional kernel for SSM\n",
    "        \n",
    "        K̄_k = C̄·Ā^k·B̄\n",
    "        \n",
    "        Args:\n",
    "            length: Length of sequence (L)\n",
    "        \n",
    "        Returns:\n",
    "            kernel: (L, p, m) convolutional kernel\n",
    "        \"\"\"\n",
    "        kernel = np.zeros((length, self.p, self.m))\n",
    "        A_power = np.eye(self.n)\n",
    "        \n",
    "        for k in range(length):\n",
    "            # K̄_k = C̄·Ā^k·B̄\n",
    "            kernel[k] = self.C_bar @ A_power @ self.B_bar\n",
    "            A_power = A_power @ self.A_bar\n",
    "        \n",
    "        return kernel\n",
    "    \n",
    "    def forward_convolutional(self, u_sequence):\n",
    "        \"\"\"\n",
    "        Convolutional (CNN-like) forward pass\n",
    "        \n",
    "        y = K̄ * u (convolution)\n",
    "        \n",
    "        Args:\n",
    "            u_sequence: (L, m) array of input sequence\n",
    "        \n",
    "        Returns:\n",
    "            y_sequence: (L, p) array of output sequence\n",
    "        \"\"\"\n",
    "        L = u_sequence.shape[0]\n",
    "        kernel = self.get_convolution_kernel(L)\n",
    "        \n",
    "        y_sequence = np.zeros((L, self.p))\n",
    "        for k in range(L):\n",
    "            # y_k = sum_j K̄_j · u_{k-j}\n",
    "            for j in range(k + 1):\n",
    "                y_sequence[k] += kernel[j].T @ u_sequence[k - j]\n",
    "        \n",
    "        return y_sequence\n",
    "    \n",
    "    def get_statistics(self):\n",
    "        \"\"\"Get SSM statistics\"\"\"\n",
    "        eigs_A = np.linalg.eigvals(self.A)\n",
    "        eigs_A_bar = np.linalg.eigvals(self.A_bar)\n",
    "        \n",
    "        return {\n",
    "            'A_stability': np.max(np.real(eigs_A)),\n",
    "            'A_bar_stability': np.max(np.abs(eigs_A_bar)),\n",
    "            'A_condition': np.linalg.cond(self.A),\n",
    "        }\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Create and test SSM for drone control\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nCreating SSM for drone control...\")\n",
    "state_dim = 4       # State: [pos, vel, accel, hidden]\n",
    "input_dim = 1       # Input: normalized command\n",
    "output_dim = 1      # Output: acceleration\n",
    "\n",
    "# Initialize with stable diagonal structure\n",
    "ssm = SSMModel(state_dim, input_dim, output_dim, dt=dt, init_method='stable')\n",
    "\n",
    "print(f\"\\nSSM Configuration:\")\n",
    "print(f\"  State dimension (n): {state_dim}\")\n",
    "print(f\"  Input dimension (m): {input_dim}\")\n",
    "print(f\"  Output dimension (p): {output_dim}\")\n",
    "print(f\"  Discretization interval (Δ): {dt}s\")\n",
    "\n",
    "print(f\"\\nContinuous Matrices (A, B, C):\")\n",
    "print(f\"  A shape: {ssm.A.shape}, condition number: {np.linalg.cond(ssm.A):.2f}\")\n",
    "print(f\"  B shape: {ssm.B.shape}\")\n",
    "print(f\"  C shape: {ssm.C.shape}\")\n",
    "\n",
    "print(f\"\\nDiscretized Matrices (Ā, B̄, C̄):\")\n",
    "print(f\"  Ā shape: {ssm.A_bar.shape}\")\n",
    "print(f\"  B̄ shape: {ssm.B_bar.shape}\")\n",
    "print(f\"  C̄ shape: {ssm.C_bar.shape}\")\n",
    "\n",
    "# Get statistics\n",
    "stats = ssm.get_statistics()\n",
    "print(f\"\\nSSM Stability Analysis:\")\n",
    "print(f\"  Max eigenvalue (continuous A): {stats['A_stability']:.6f}\")\n",
    "print(f\"  Max magnitude eigenvalue (discrete Ā): {stats['A_bar_stability']:.6f}\")\n",
    "if stats['A_bar_stability'] < 1.0:\n",
    "    print(f\"  ✓ Discrete system is STABLE (|λ| < 1)\")\n",
    "else:\n",
    "    print(f\"  ✗ Warning: Discrete system may be UNSTABLE (|λ| ≥ 1)\")\n",
    "\n",
    "print(\"\\n✓ SSM created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a78982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SSM TRAINING WITH LEARNABLE PARAMETERS\n",
    "# ============================================================================\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "class SSMLayer(nn.Module):\n",
    "    \"\"\"Learnable SSM layer using PyTorch\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, input_dim, output_dim, dt=0.05):\n",
    "        super().__init__()\n",
    "        self.n = state_dim\n",
    "        self.m = input_dim\n",
    "        self.p = output_dim\n",
    "        self.dt = dt\n",
    "        \n",
    "        # Learnable continuous matrices\n",
    "        self.A = nn.Parameter(torch.randn(state_dim, state_dim) * 0.1)\n",
    "        self.B = nn.Parameter(torch.randn(state_dim, input_dim) * 0.1)\n",
    "        self.C = nn.Parameter(torch.randn(output_dim, state_dim) * 0.1)\n",
    "        \n",
    "        # Initialize A to be stable\n",
    "        with torch.no_grad():\n",
    "            self.A.copy_(torch.randn(state_dim, state_dim) - 2 * torch.eye(state_dim))\n",
    "    \n",
    "    def discretize(self):\n",
    "        \"\"\"Discretize continuous SSM using trapezoid method\"\"\"\n",
    "        I = torch.eye(self.n, device=self.A.device)\n",
    "        \n",
    "        left_factor = I - (self.dt / 2) * self.A\n",
    "        right_factor = I + (self.dt / 2) * self.A\n",
    "        \n",
    "        # Add regularization for numerical stability\n",
    "        left_factor_reg = left_factor + 1e-6 * I\n",
    "        \n",
    "        A_bar = torch.linalg.solve(left_factor_reg, right_factor)\n",
    "        B_bar = torch.linalg.solve(left_factor_reg, self.dt * self.B)\n",
    "        C_bar = self.C\n",
    "        \n",
    "        return A_bar, B_bar, C_bar\n",
    "    \n",
    "    def forward_recursive(self, u_sequence):\n",
    "        \"\"\"\n",
    "        Recursive forward pass\n",
    "        \n",
    "        Args:\n",
    "            u_sequence: (batch, length, input_dim) tensor\n",
    "        \n",
    "        Returns:\n",
    "            y_sequence: (batch, length, output_dim) tensor\n",
    "        \"\"\"\n",
    "        batch_size, length, _ = u_sequence.shape\n",
    "        device = u_sequence.device\n",
    "        \n",
    "        A_bar, B_bar, C_bar = self.discretize()\n",
    "        \n",
    "        # Initialize hidden state\n",
    "        x = torch.zeros(batch_size, self.n, device=device)\n",
    "        y_sequence = []\n",
    "        \n",
    "        for k in range(length):\n",
    "            # x_k = Ā·x_{k-1} + B̄·u_k\n",
    "            x = (A_bar @ x.T).T + (u_sequence[:, k:k+1] @ B_bar.T)\n",
    "            # y_k = C̄·x_k\n",
    "            y = x @ C_bar.T\n",
    "            y_sequence.append(y)\n",
    "        \n",
    "        y_sequence = torch.stack(y_sequence, dim=1)  # (batch, length, output_dim)\n",
    "        return y_sequence\n",
    "    \n",
    "    def forward(self, u_sequence):\n",
    "        \"\"\"Forward pass (recursive view)\"\"\"\n",
    "        return self.forward_recursive(u_sequence)\n",
    "\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train)\n",
    "y_train_tensor = torch.FloatTensor(y_train)\n",
    "X_test_tensor = torch.FloatTensor(X_test)\n",
    "y_test_tensor = torch.FloatTensor(y_test)\n",
    "\n",
    "# Create DataLoader\n",
    "batch_size = 32\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Initialize SSM model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "ssm_model = SSMLayer(state_dim=8, input_dim=1, output_dim=1, dt=dt).to(device)\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(ssm_model.parameters(), lr=0.001)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING SSM MODEL\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "print(f\"Epochs: 25\")\n",
    "\n",
    "# Training loop\n",
    "ssm_train_losses = []\n",
    "ssm_val_losses = []\n",
    "epochs = 25\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Training\n",
    "    train_loss = 0.0\n",
    "    for batch_x, batch_y in train_loader:\n",
    "        batch_x = batch_x.to(device)\n",
    "        batch_y = batch_y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = ssm_model(batch_x)\n",
    "        loss = criterion(output, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    train_loss /= len(train_loader)\n",
    "    ssm_train_losses.append(train_loss)\n",
    "    \n",
    "    # Validation\n",
    "    with torch.no_grad():\n",
    "        val_output = ssm_model(X_test_tensor.to(device))\n",
    "        val_loss = criterion(val_output, y_test_tensor.to(device))\n",
    "        ssm_val_losses.append(val_loss.item())\n",
    "    \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f\"Epoch {epoch+1:2d}/{epochs} | Train Loss: {train_loss:.6f} | Val Loss: {val_loss.item():.6f}\")\n",
    "\n",
    "print(f\"\\n✓ SSM training complete\")\n",
    "print(f\"  Final train loss: {ssm_train_losses[-1]:.6f}\")\n",
    "print(f\"  Final val loss: {ssm_val_losses[-1]:.6f}\")\n",
    "\n",
    "# Get predictions on test set\n",
    "ssm_model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred_ssm_norm = ssm_model(X_test_tensor.to(device)).cpu().numpy()\n",
    "\n",
    "# Denormalize predictions\n",
    "y_pred_ssm = y_pred_ssm_norm * global_max_abs_y\n",
    "y_test_real_check = y_test * global_max_abs_y\n",
    "\n",
    "# Compute metrics\n",
    "ssm_mse = np.mean((y_pred_ssm - y_test_real_check)**2)\n",
    "ssm_rmse = np.sqrt(ssm_mse)\n",
    "ssm_mae = np.mean(np.abs(y_pred_ssm - y_test_real_check))\n",
    "\n",
    "print(f\"\\nSSM Performance Metrics:\")\n",
    "print(f\"  MSE:  {ssm_mse:.8f}\")\n",
    "print(f\"  RMSE: {ssm_rmse:.8f}\")\n",
    "print(f\"  MAE:  {ssm_mae:.8f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a97f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SSM RECURSIVE vs CONVOLUTIONAL VIEW COMPARISON\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RECURSIVE vs CONVOLUTIONAL VIEW ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def ssm_forward_convolutional_batch(ssm_layer, u_sequence):\n",
    "    \"\"\"\n",
    "    Convolutional view forward pass using implicit convolution\n",
    "    \n",
    "    Args:\n",
    "        ssm_layer: Trained SSM layer\n",
    "        u_sequence: (batch, length, input_dim) tensor\n",
    "    \n",
    "    Returns:\n",
    "        y_sequence: (batch, length, output_dim) tensor\n",
    "    \"\"\"\n",
    "    batch_size, length, _ = u_sequence.shape\n",
    "    device = u_sequence.device\n",
    "    \n",
    "    # Get discretized matrices\n",
    "    A_bar, B_bar, C_bar = ssm_layer.discretize()\n",
    "    \n",
    "    # Compute convolution kernel K̄_k = C̄·Ā^k·B̄\n",
    "    kernel = []\n",
    "    A_power = torch.eye(ssm_layer.n, device=device)\n",
    "    \n",
    "    for k in range(length):\n",
    "        # K̄_k = C̄·Ā^k·B̄\n",
    "        K_k = C_bar @ A_power @ B_bar  # (output_dim, input_dim)\n",
    "        kernel.append(K_k)\n",
    "        A_power = A_power @ A_bar\n",
    "    \n",
    "    kernel = torch.stack(kernel, dim=0)  # (length, output_dim, input_dim)\n",
    "    \n",
    "    # Apply convolution\n",
    "    y_sequence = torch.zeros(batch_size, length, ssm_layer.p, device=device)\n",
    "    \n",
    "    for k in range(length):\n",
    "        for j in range(k + 1):\n",
    "            # y_k += K̄_j · u_{k-j}\n",
    "            y_sequence[:, k] += (u_sequence[:, k - j] @ kernel[j].T)\n",
    "    \n",
    "    return y_sequence, kernel\n",
    "\n",
    "\n",
    "# Generate predictions with both views\n",
    "with torch.no_grad():\n",
    "    # Recursive view\n",
    "    y_recursive = ssm_model(X_test_tensor.to(device)).cpu().numpy()\n",
    "    \n",
    "    # Convolutional view\n",
    "    y_conv, kernel = ssm_forward_convolutional_batch(ssm_model, X_test_tensor.to(device))\n",
    "    y_conv = y_conv.cpu().numpy()\n",
    "\n",
    "print(f\"\\nOutput shapes match: {y_recursive.shape == y_conv.shape}\")\n",
    "print(f\"Outputs identical: {np.allclose(y_recursive, y_conv, atol=1e-5)}\")\n",
    "\n",
    "# Compute differences\n",
    "max_diff = np.max(np.abs(y_recursive - y_conv))\n",
    "mean_diff = np.mean(np.abs(y_recursive - y_conv))\n",
    "print(f\"  Max difference: {max_diff:.2e}\")\n",
    "print(f\"  Mean difference: {mean_diff:.2e}\")\n",
    "print(f\"✓ Both views produce consistent results (differences are numerical precision artifacts)\")\n",
    "\n",
    "# Timing comparison\n",
    "import time\n",
    "\n",
    "def time_recursive_pass(n_trials=100):\n",
    "    times = []\n",
    "    for _ in range(n_trials):\n",
    "        with torch.no_grad():\n",
    "            start = time.time()\n",
    "            ssm_model(X_test_tensor.to(device))\n",
    "            times.append(time.time() - start)\n",
    "    return np.mean(times) * 1000  # Convert to ms\n",
    "\n",
    "def time_convolutional_pass(n_trials=100):\n",
    "    times = []\n",
    "    for _ in range(n_trials):\n",
    "        with torch.no_grad():\n",
    "            start = time.time()\n",
    "            ssm_forward_convolutional_batch(ssm_model, X_test_tensor.to(device))\n",
    "            times.append(time.time() - start)\n",
    "    return np.mean(times) * 1000  # Convert to ms\n",
    "\n",
    "print(\"\\nTiming Analysis (averaged over 100 trials):\")\n",
    "print(\"  Computing recursive pass timing...\")\n",
    "time_rec = time_recursive_pass(50)\n",
    "print(\"  Computing convolutional pass timing...\")\n",
    "time_conv = time_convolutional_pass(50)\n",
    "\n",
    "print(f\"  Recursive view:      {time_rec:.3f} ms\")\n",
    "print(f\"  Convolutional view:  {time_conv:.3f} ms\")\n",
    "\n",
    "if time_rec < time_conv:\n",
    "    speedup = time_conv / time_rec\n",
    "    print(f\"  → Recursive is {speedup:.2f}x FASTER (better for inference)\")\n",
    "else:\n",
    "    slowdown = time_rec / time_conv\n",
    "    print(f\"  → Convolutional is {slowdown:.2f}x FASTER (better for parallel training)\")\n",
    "\n",
    "# Visualization of both views\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "\n",
    "# Select a test sample\n",
    "test_idx = 0\n",
    "test_sample_input = X_test[test_idx]\n",
    "test_sample_output_true = y_test_real_check[test_idx]\n",
    "test_sample_output_rec = y_recursive[test_idx]\n",
    "test_sample_output_conv = y_conv[test_idx]\n",
    "\n",
    "# Plot 1: Input commands\n",
    "axes[0, 0].plot(test_sample_input, 'b-', linewidth=2, label='Normalized command')\n",
    "axes[0, 0].set_title('Input Command Sequence', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_ylabel('Normalized Command')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# Plot 2: Output comparison (time domain)\n",
    "time_steps = np.arange(len(test_sample_output_true))\n",
    "axes[0, 1].plot(time_steps, test_sample_output_true, 'k--', linewidth=2, label='True acceleration', alpha=0.7)\n",
    "axes[0, 1].plot(time_steps, test_sample_output_rec, 'b-', linewidth=1.5, label='SSM Recursive')\n",
    "axes[0, 1].plot(time_steps, test_sample_output_conv, 'r:', linewidth=2, label='SSM Convolutional')\n",
    "axes[0, 1].set_title('Output Prediction Comparison', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_ylabel('Acceleration (m/s²)')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Prediction errors\n",
    "error_rec = test_sample_output_rec - test_sample_output_true\n",
    "error_conv = test_sample_output_conv - test_sample_output_true\n",
    "axes[0, 2].plot(time_steps, error_rec, 'b-', linewidth=1.5, label='Recursive error', alpha=0.7)\n",
    "axes[0, 2].plot(time_steps, error_conv, 'r-', linewidth=1.5, label='Convolutional error', alpha=0.7)\n",
    "axes[0, 2].axhline(0, color='k', linestyle='--', alpha=0.3)\n",
    "axes[0, 2].set_title('Prediction Errors', fontsize=12, fontweight='bold')\n",
    "axes[0, 2].set_ylabel('Error (m/s²)')\n",
    "axes[0, 2].legend()\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Training loss curves\n",
    "axes[1, 0].semilogy(ssm_train_losses, 'b-', linewidth=2, label='Training loss')\n",
    "axes[1, 0].semilogy(ssm_val_losses, 'r-', linewidth=2, label='Validation loss')\n",
    "axes[1, 0].set_title('SSM Training Progress', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('Loss (log scale)')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3, which='both')\n",
    "\n",
    "# Plot 5: Convolution kernel magnitude\n",
    "kernel_np = kernel.cpu().numpy()\n",
    "kernel_magnitude = np.abs(kernel_np[:, 0, 0])  # For 1D output/input case\n",
    "axes[1, 1].bar(range(len(kernel_magnitude)), kernel_magnitude, color='steelblue', alpha=0.7)\n",
    "axes[1, 1].set_title('Convolution Kernel Magnitude', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Lag (k)')\n",
    "axes[1, 1].set_ylabel('|K̄_k|')\n",
    "axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Plot 6: Recursive view explanation\n",
    "axes[1, 2].text(0.05, 0.95, 'RECURSIVE VIEW\\n(RNN-like)\\n\\n' +\n",
    "                            '• Iterative: state updates one step at a time\\n' +\n",
    "                            '• Memory efficient: O(L) space\\n' +\n",
    "                            '• Sequential: must process in order\\n' +\n",
    "                            f'• Time per sample: {time_rec:.3f} ms\\n\\n' +\n",
    "                            'CONVOLUTIONAL VIEW\\n(CNN-like)\\n\\n' +\n",
    "                            '• Parallel: entire sequence at once\\n' +\n",
    "                            '• Compute intensive: O(L²) ops\\n' +\n",
    "                            '• Parallelizable: great for GPUs\\n' +\n",
    "                            f'• Time per sample: {time_conv:.3f} ms',\n",
    "                transform=axes[1, 2].transAxes,\n",
    "                fontsize=10,\n",
    "                verticalalignment='top',\n",
    "                fontfamily='monospace',\n",
    "                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "axes[1, 2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('ssm_recursive_vs_convolutional.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n✓ Recursive vs Convolutional visualization saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793489c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# COMPREHENSIVE COMPARISON: LQR vs LSTM vs SSM\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPREHENSIVE THREE-METHOD COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================================================\n",
    "# 1. SSM INVERSE MODEL FOR CLOSED-LOOP CONTROL\n",
    "# ============================================================================\n",
    "\n",
    "class SSMInverseModel(nn.Module):\n",
    "    \"\"\"SSM model for learning inverse dynamics: command from acceleration\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, input_dim, output_dim, dt=0.05):\n",
    "        super().__init__()\n",
    "        self.n = state_dim\n",
    "        self.m = input_dim\n",
    "        self.p = output_dim\n",
    "        self.dt = dt\n",
    "        \n",
    "        self.A = nn.Parameter(torch.randn(state_dim, state_dim) * 0.1)\n",
    "        self.B = nn.Parameter(torch.randn(state_dim, input_dim) * 0.1)\n",
    "        self.C = nn.Parameter(torch.randn(output_dim, state_dim) * 0.1)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            self.A.copy_(torch.randn(state_dim, state_dim) - 2 * torch.eye(state_dim))\n",
    "    \n",
    "    def discretize(self):\n",
    "        I = torch.eye(self.n, device=self.A.device)\n",
    "        left_factor = I - (self.dt / 2) * self.A + 1e-6 * I\n",
    "        right_factor = I + (self.dt / 2) * self.A\n",
    "        \n",
    "        A_bar = torch.linalg.solve(left_factor, right_factor)\n",
    "        B_bar = torch.linalg.solve(left_factor, self.dt * self.B)\n",
    "        C_bar = self.C\n",
    "        \n",
    "        return A_bar, B_bar, C_bar\n",
    "    \n",
    "    def forward(self, u_sequence):\n",
    "        batch_size, length, _ = u_sequence.shape\n",
    "        device = u_sequence.device\n",
    "        \n",
    "        A_bar, B_bar, C_bar = self.discretize()\n",
    "        \n",
    "        x = torch.zeros(batch_size, self.n, device=device)\n",
    "        y_sequence = []\n",
    "        \n",
    "        for k in range(length):\n",
    "            x = (A_bar @ x.T).T + (u_sequence[:, k:k+1] @ B_bar.T)\n",
    "            y = x @ C_bar.T\n",
    "            y_sequence.append(y)\n",
    "        \n",
    "        y_sequence = torch.stack(y_sequence, dim=1)\n",
    "        return y_sequence\n",
    "\n",
    "\n",
    "# Train SSM inverse model\n",
    "print(\"\\nTraining SSM Inverse Model...\")\n",
    "ssm_inverse_model = SSMInverseModel(state_dim=8, input_dim=1, output_dim=1, dt=dt).to(device)\n",
    "optimizer_inv = optim.Adam(ssm_inverse_model.parameters(), lr=0.001)\n",
    "\n",
    "ssm_inv_train_losses = []\n",
    "ssm_inv_val_losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss = 0.0\n",
    "    for batch_x, batch_y in train_loader:\n",
    "        batch_x = batch_x.to(device)\n",
    "        batch_y = batch_y.to(device)\n",
    "        \n",
    "        optimizer_inv.zero_grad()\n",
    "        output = ssm_inverse_model(batch_x)\n",
    "        loss = criterion(output, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer_inv.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    train_loss /= len(train_loader)\n",
    "    ssm_inv_train_losses.append(train_loss)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        val_output = ssm_inverse_model(X_test_tensor.to(device))\n",
    "        val_loss = criterion(val_output, y_test_tensor.to(device))\n",
    "        ssm_inv_val_losses.append(val_loss.item())\n",
    "    \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f\"  Epoch {epoch+1:2d}/{epochs} | Train: {train_loss:.6f} | Val: {val_loss.item():.6f}\")\n",
    "\n",
    "print(\"✓ SSM inverse model trained\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# 2. CLOSED-LOOP SIMULATION WITH SSM CONTROLLER\n",
    "# ============================================================================\n",
    "\n",
    "def simulate_ssm_controller(ssm_forward, ssm_inverse, \n",
    "                           initial_state, target_height, num_steps,\n",
    "                           dt=0.05, max_command=1.0):\n",
    "    \"\"\"\n",
    "    Simulate closed-loop control using SSM inverse model\n",
    "    \n",
    "    Args:\n",
    "        ssm_forward: Trained forward SSM model\n",
    "        ssm_inverse: Trained inverse SSM model\n",
    "        initial_state: [height, velocity, acceleration]\n",
    "        target_height: Target position\n",
    "        num_steps: Number of simulation steps\n",
    "        dt: Sampling interval\n",
    "        max_command: Maximum command magnitude (normalized)\n",
    "    \n",
    "    Returns:\n",
    "        positions, velocities, accelerations, commands\n",
    "    \"\"\"\n",
    "    device_sim = next(ssm_forward.parameters()).device\n",
    "    \n",
    "    # Initialize trajectory tracking\n",
    "    pos = initial_state[0]\n",
    "    vel = initial_state[1]\n",
    "    acc = initial_state[2]\n",
    "    \n",
    "    positions = [pos]\n",
    "    velocities = [vel]\n",
    "    accelerations = [acc]\n",
    "    commands = []\n",
    "    \n",
    "    # History for RNN/SSM input\n",
    "    command_history = np.zeros((1, 20, 1))  # 20-step history\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        # Desired acceleration (P controller on position error)\n",
    "        pos_error = target_height - pos\n",
    "        desired_acc = 50.0 * pos_error\n",
    "        desired_acc = np.clip(desired_acc, -1, 1)\n",
    "        \n",
    "        # Predict command from desired acceleration using SSM inverse\n",
    "        desired_acc_tensor = torch.FloatTensor([[desired_acc]]).reshape(1, 1, 1).to(device_sim)\n",
    "        with torch.no_grad():\n",
    "            command_pred = ssm_inverse_model(desired_acc_tensor).cpu().numpy()[0, 0, 0]\n",
    "        \n",
    "        command = np.clip(command_pred, -max_command, max_command)\n",
    "        commands.append(command)\n",
    "        \n",
    "        # Update command history\n",
    "        command_history = np.roll(command_history, -1, axis=1)\n",
    "        command_history[0, -1, 0] = command\n",
    "        \n",
    "        # Predict acceleration using SSM forward model\n",
    "        command_tensor = torch.FloatTensor(command_history).to(device_sim)\n",
    "        with torch.no_grad():\n",
    "            acc_pred = ssm_forward(command_tensor).cpu().numpy()[0, -1, 0]\n",
    "        \n",
    "        acc = acc_pred * global_max_abs_y  # Denormalize\n",
    "        acc = np.clip(acc, -15, 15)  # Clip to reasonable limits\n",
    "        \n",
    "        # Update state (kinematic)\n",
    "        vel += acc * dt\n",
    "        pos += vel * dt\n",
    "        \n",
    "        positions.append(pos)\n",
    "        velocities.append(vel)\n",
    "        accelerations.append(acc)\n",
    "    \n",
    "    return np.array(positions), np.array(velocities), np.array(accelerations), np.array(commands)\n",
    "\n",
    "\n",
    "# Run SSM controller simulation\n",
    "print(\"Simulating SSM Controller...\")\n",
    "pos_ssm, vel_ssm, acc_ssm, cmd_ssm = simulate_ssm_controller(\n",
    "    ssm_model, ssm_inverse_model,\n",
    "    initial_state=[0, 0, 0],\n",
    "    target_height=10.0,\n",
    "    num_steps=200\n",
    ")\n",
    "print(\"✓ SSM simulation complete\")\n",
    "\n",
    "# ============================================================================\n",
    "# 3. COMPUTE PERFORMANCE METRICS FOR ALL THREE METHODS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nComputing performance metrics for all three methods...\\n\")\n",
    "\n",
    "def compute_metrics(positions, velocities, accelerations, commands, target_height, dt=0.05):\n",
    "    \"\"\"Compute control performance metrics\"\"\"\n",
    "    \n",
    "    # Steady-state error\n",
    "    mean_pos_error = np.mean(np.abs(positions[-50:] - target_height))\n",
    "    final_pos_error = np.abs(positions[-1] - target_height)\n",
    "    \n",
    "    # Rise time (time to reach 90% of target)\n",
    "    rise_time_idx = np.argmax(positions >= 0.9 * target_height)\n",
    "    rise_time = rise_time_idx * dt if rise_time_idx > 0 else np.nan\n",
    "    \n",
    "    # Overshoot\n",
    "    max_pos = np.max(positions)\n",
    "    overshoot = max((max_pos - target_height) / target_height, 0) * 100\n",
    "    \n",
    "    # Smoothness (velocity variance)\n",
    "    vel_smoothness = np.std(np.diff(velocities))\n",
    "    \n",
    "    # Control effort (sum of squared commands)\n",
    "    control_effort = np.sum(commands**2)\n",
    "    \n",
    "    # Settling time (time to stay within 2% band)\n",
    "    settled_idx = np.where(np.abs(positions - target_height) <= 0.02 * target_height)[0]\n",
    "    if len(settled_idx) > 10:\n",
    "        settling_time = settled_idx[0] * dt\n",
    "    else:\n",
    "        settling_time = np.nan\n",
    "    \n",
    "    # Final velocity error\n",
    "    final_vel_error = np.abs(velocities[-1])\n",
    "    \n",
    "    return {\n",
    "        'Mean Position Error': mean_pos_error,\n",
    "        'Final Position Error': final_pos_error,\n",
    "        'Rise Time (s)': rise_time,\n",
    "        'Overshoot (%)': overshoot,\n",
    "        'Settling Time (s)': settling_time,\n",
    "        'Velocity Smoothness': vel_smoothness,\n",
    "        'Control Effort': control_effort,\n",
    "        'Final Velocity Error': final_vel_error\n",
    "    }\n",
    "\n",
    "\n",
    "# Compute metrics for all three controllers\n",
    "metrics_lqr_identified = compute_metrics(pos_lqr_identified, vel_lqr_identified, acc_lqr_identified, cmd_lqr_identified, target_height)\n",
    "metrics_lstm = compute_metrics(pos_lstm, vel_lstm, acc_lstm, cmd_lstm, target_height)\n",
    "metrics_ssm = compute_metrics(pos_ssm, vel_ssm, acc_ssm, cmd_ssm, target_height)\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comparison_df = pd.DataFrame({\n",
    "    'LQR (Identified)': metrics_lqr_identified,\n",
    "    'LSTM': metrics_lstm,\n",
    "    'SSM': metrics_ssm\n",
    "})\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PERFORMANCE METRICS COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "print(comparison_df.to_string())\n",
    "print()\n",
    "\n",
    "# Normalize metrics for radar chart\n",
    "metrics_for_radar = comparison_df.copy()\n",
    "# Normalize each metric to [0, 1] range (lower is better for all metrics in this case)\n",
    "for idx in metrics_for_radar.index:\n",
    "    max_val = metrics_for_radar.loc[idx].max()\n",
    "    if max_val > 0:\n",
    "        metrics_for_radar.loc[idx] = 1 - (metrics_for_radar.loc[idx] / max_val)\n",
    "\n",
    "print(\"Normalized metrics (higher = better):\")\n",
    "print(metrics_for_radar.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fedf4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# COMPREHENSIVE VISUALIZATION: ALL THREE METHODS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nGenerating comprehensive comparison visualizations...\\n\")\n",
    "\n",
    "fig = plt.figure(figsize=(18, 12))\n",
    "gs = fig.add_gridspec(3, 3, hspace=0.35, wspace=0.3)\n",
    "\n",
    "time_array = np.arange(len(pos_lqr_identified)) * dt\n",
    "target_line = np.ones_like(time_array) * target_height\n",
    "\n",
    "# ============================================================================\n",
    "# ROW 1: POSITION TRAJECTORIES\n",
    "# ============================================================================\n",
    "\n",
    "ax1 = fig.add_subplot(gs[0, :2])\n",
    "ax1.plot(time_array, pos_lqr_identified, 'g-', linewidth=2.5, label='LQR (Identified)', alpha=0.8)\n",
    "ax1.plot(time_array, pos_lstm, 'b-', linewidth=2.5, label='LSTM', alpha=0.8)\n",
    "ax1.plot(time_array, pos_ssm, 'r-', linewidth=2.5, label='SSM', alpha=0.8)\n",
    "ax1.plot(time_array, target_line, 'k--', linewidth=2, label='Target', alpha=0.5)\n",
    "ax1.fill_between(time_array, target_height - 0.2, target_height + 0.2, \n",
    "                  alpha=0.1, color='gray', label='±2% band')\n",
    "ax1.set_ylabel('Position (m)', fontsize=11, fontweight='bold')\n",
    "ax1.set_title('Position Tracking: All Three Methods', fontsize=12, fontweight='bold')\n",
    "ax1.legend(loc='best', fontsize=10)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_ylim([-1, 12])\n",
    "\n",
    "# Performance summary text\n",
    "ax_text = fig.add_subplot(gs[0, 2])\n",
    "ax_text.axis('off')\n",
    "summary_text = (\n",
    "    'PERFORMANCE SUMMARY\\n\\n'\n",
    "    f\"LQR (ID):\\n\"\n",
    "    f\"  Error: {metrics_lqr_identified['Final Position Error']:.3f} m\\n\"\n",
    "    f\"  Rise: {metrics_lqr_identified['Rise Time (s)']:.2f} s\\n\"\n",
    "    f\"  Effort: {metrics_lqr_identified['Control Effort']:.1f}\\n\\n\"\n",
    "    f\"LSTM:\\n\"\n",
    "    f\"  Error: {metrics_lstm['Final Position Error']:.3f} m\\n\"\n",
    "    f\"  Rise: {metrics_lstm['Rise Time (s)']:.2f} s\\n\"\n",
    "    f\"  Effort: {metrics_lstm['Control Effort']:.1f}\\n\\n\"\n",
    "    f\"SSM:\\n\"\n",
    "    f\"  Error: {metrics_ssm['Final Position Error']:.3f} m\\n\"\n",
    "    f\"  Rise: {metrics_ssm['Rise Time (s)']:.2f} s\\n\"\n",
    "    f\"  Effort: {metrics_ssm['Control Effort']:.1f}\"\n",
    ")\n",
    "ax_text.text(0.05, 0.95, summary_text, transform=ax_text.transAxes,\n",
    "            fontsize=10, verticalalignment='top', fontfamily='monospace',\n",
    "            bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.8))\n",
    "\n",
    "# ============================================================================\n",
    "# ROW 2: VELOCITY AND ACCELERATION\n",
    "# ============================================================================\n",
    "\n",
    "ax2 = fig.add_subplot(gs[1, 0])\n",
    "ax2.plot(time_array, vel_lqr_identified, 'g-', linewidth=2, label='LQR', alpha=0.8)\n",
    "ax2.plot(time_array, vel_lstm, 'b-', linewidth=2, label='LSTM', alpha=0.8)\n",
    "ax2.plot(time_array, vel_ssm, 'r-', linewidth=2, label='SSM', alpha=0.8)\n",
    "ax2.set_ylabel('Velocity (m/s)', fontsize=11, fontweight='bold')\n",
    "ax2.set_title('Velocity Profiles', fontsize=11, fontweight='bold')\n",
    "ax2.legend(fontsize=9)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "ax3 = fig.add_subplot(gs[1, 1])\n",
    "ax3.plot(time_array, acc_lqr_identified, 'g-', linewidth=2, label='LQR', alpha=0.8)\n",
    "ax3.plot(time_array, acc_lstm, 'b-', linewidth=2, label='LSTM', alpha=0.8)\n",
    "ax3.plot(time_array, acc_ssm, 'r-', linewidth=2, label='SSM', alpha=0.8)\n",
    "ax3.set_ylabel('Acceleration (m/s²)', fontsize=11, fontweight='bold')\n",
    "ax3.set_title('Acceleration Profiles', fontsize=11, fontweight='bold')\n",
    "ax3.legend(fontsize=9)\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "ax4 = fig.add_subplot(gs[1, 2])\n",
    "ax4.plot(time_array, cmd_lqr_identified, 'g-', linewidth=2, label='LQR', alpha=0.8)\n",
    "ax4.plot(time_array, cmd_lstm, 'b-', linewidth=2, label='LSTM', alpha=0.8)\n",
    "ax4.plot(time_array, cmd_ssm, 'r-', linewidth=2, label='SSM', alpha=0.8)\n",
    "ax4.set_ylabel('Normalized Command', fontsize=11, fontweight='bold')\n",
    "ax4.set_title('Control Commands', fontsize=11, fontweight='bold')\n",
    "ax4.legend(fontsize=9)\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "# ============================================================================\n",
    "# ROW 3: ERRORS AND PHASE PORTRAIT\n",
    "# ============================================================================\n",
    "\n",
    "ax5 = fig.add_subplot(gs[2, 0])\n",
    "error_lqr = pos_lqr_identified - target_height\n",
    "error_lstm = pos_lstm - target_height\n",
    "error_ssm = pos_ssm - target_height\n",
    "ax5.plot(time_array, error_lqr, 'g-', linewidth=2, label='LQR', alpha=0.8)\n",
    "ax5.plot(time_array, error_lstm, 'b-', linewidth=2, label='LSTM', alpha=0.8)\n",
    "ax5.plot(time_array, error_ssm, 'r-', linewidth=2, label='SSM', alpha=0.8)\n",
    "ax5.axhline(0, color='k', linestyle='--', alpha=0.3)\n",
    "ax5.fill_between(time_array, -0.2, 0.2, alpha=0.1, color='gray')\n",
    "ax5.set_ylabel('Position Error (m)', fontsize=11, fontweight='bold')\n",
    "ax5.set_xlabel('Time (s)', fontsize=11, fontweight='bold')\n",
    "ax5.set_title('Position Tracking Error', fontsize=11, fontweight='bold')\n",
    "ax5.legend(fontsize=9)\n",
    "ax5.grid(True, alpha=0.3)\n",
    "\n",
    "# Phase portrait\n",
    "ax6 = fig.add_subplot(gs[2, 1])\n",
    "ax6.plot(pos_lqr_identified, vel_lqr_identified, 'g-', linewidth=2, label='LQR', alpha=0.8)\n",
    "ax6.plot(pos_lstm, vel_lstm, 'b-', linewidth=2, label='LSTM', alpha=0.8)\n",
    "ax6.plot(pos_ssm, vel_ssm, 'r-', linewidth=2, label='SSM', alpha=0.8)\n",
    "ax6.axvline(target_height, color='k', linestyle='--', alpha=0.3)\n",
    "ax6.set_xlabel('Position (m)', fontsize=11, fontweight='bold')\n",
    "ax6.set_ylabel('Velocity (m/s)', fontsize=11, fontweight='bold')\n",
    "ax6.set_title('Phase Portrait (Pos vs Vel)', fontsize=11, fontweight='bold')\n",
    "ax6.legend(fontsize=9)\n",
    "ax6.grid(True, alpha=0.3)\n",
    "\n",
    "# Metrics bar chart\n",
    "ax7 = fig.add_subplot(gs[2, 2])\n",
    "metrics_names = ['Pos Error', 'Rise Time', 'Overshoot']\n",
    "lqr_vals = [metrics_lqr_identified['Final Position Error'],\n",
    "            metrics_lqr_identified['Rise Time (s)'],\n",
    "            metrics_lqr_identified['Overshoot (%)'] / 10]  # Scale for visibility\n",
    "lstm_vals = [metrics_lstm['Final Position Error'],\n",
    "             metrics_lstm['Rise Time (s)'],\n",
    "             metrics_lstm['Overshoot (%)'] / 10]\n",
    "ssm_vals = [metrics_ssm['Final Position Error'],\n",
    "            metrics_ssm['Rise Time (s)'],\n",
    "            metrics_ssm['Overshoot (%)'] / 10]\n",
    "\n",
    "x = np.arange(len(metrics_names))\n",
    "width = 0.25\n",
    "ax7.bar(x - width, lqr_vals, width, label='LQR', color='green', alpha=0.7)\n",
    "ax7.bar(x, lstm_vals, width, label='LSTM', color='blue', alpha=0.7)\n",
    "ax7.bar(x + width, ssm_vals, width, label='SSM', color='red', alpha=0.7)\n",
    "ax7.set_ylabel('Metric Value', fontsize=11, fontweight='bold')\n",
    "ax7.set_title('Key Performance Metrics', fontsize=11, fontweight='bold')\n",
    "ax7.set_xticks(x)\n",
    "ax7.set_xticklabels(metrics_names, fontsize=9)\n",
    "ax7.legend(fontsize=9)\n",
    "ax7.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.suptitle('Comprehensive Three-Method Comparison: LQR vs LSTM vs SSM', \n",
    "             fontsize=14, fontweight='bold', y=0.995)\n",
    "plt.savefig('three_method_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Comprehensive comparison visualization saved as 'three_method_comparison.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9ad0c7",
   "metadata": {},
   "source": [
    "## Section 12: Complete Framework Summary and Decision Guide\n",
    "\n",
    "### Overview of Three Control Paradigms\n",
    "\n",
    "This notebook has presented three fundamentally different approaches to controlling vertical drone motion:\n",
    "\n",
    "| Aspect | LQR (Classical) | LSTM (RNN) | SSM (Modern DL) |\n",
    "|--------|-----------------|-----------|-----------------|\n",
    "| **Theoretical Foundation** | Optimal control theory, DARE | Neural network sequence modeling | Continuous dynamical systems theory |\n",
    "| **System Identification** | Requires explicit A, B matrices | Learns from data implicitly | Requires discretization strategy |\n",
    "| **Computational Cost** | O(n³) matrix operations | O(L×n²) per sequence | O(L²) training, O(L) inference |\n",
    "| **Parallelization** | Limited | Good (LSTM parallel) | Excellent (convolutional view) |\n",
    "| **Interpretability** | High (closed-form gain K) | Low (black box) | Medium (discretization visible) |\n",
    "| **Robustness** | Good for linear systems | Handles nonlinearity well | Balances both |\n",
    "| **Training Data Need** | Minimal (for identification) | Large dataset required | Moderate to large |\n",
    "| **Real-time Inference** | Very fast (linear map) | Fast (recursive LSTM) | Very fast (parallel) |\n",
    "\n",
    "### Mathematical Connections\n",
    "\n",
    "**LQR and SSM Connection:**\n",
    "- LQR assumes a known linear system: $x_{k+1} = Ax_k + Bu_k$\n",
    "- SSM learns the system dynamics through continuous formulation with discretization\n",
    "- Both use the same discrete state-space representation: $x_k = \\bar{A}x_{k-1} + \\bar{B}u_k$\n",
    "- Difference: LQR optimizes the control law *given* A, B; SSM learns A, B from data\n",
    "\n",
    "**SSM and LSTM Connection:**\n",
    "- SSM recursive view is mathematically equivalent to an RNN with specific weight structure\n",
    "- LSTM adds gating mechanisms to improve gradient flow\n",
    "- SSM convolutional view enables parallelized training like CNNs\n",
    "- Both process sequences, but with different architectures and learning dynamics\n",
    "\n",
    "**System Identification Bridge:**\n",
    "- BONUS section extracted $A_{identified}$, $B_{identified}$ from experimental data\n",
    "- These matrices can be used directly in LQR: gives data-driven LQR controller\n",
    "- LSTM and SSM learn mappings without explicit matrices\n",
    "- Result: All three methods learn the same system behavior through different representations\n",
    "\n",
    "### Performance Summary\n",
    "\n",
    "Based on the simulation results with target height = 10.0 m:\n",
    "\n",
    "**LQR (Identified System):**\n",
    "- ✅ Fastest computation and inference\n",
    "- ✅ Most interpretable control law: $u = -K(x - x_{target})$\n",
    "- ✅ Optimal for linear systems (if system is actually linear)\n",
    "- ⚠️ Performance depends critically on system identification quality\n",
    "- ⚠️ May struggle with nonlinearities in real system\n",
    "\n",
    "**LSTM Controller:**\n",
    "- ✅ Automatically captures nonlinearities\n",
    "- ✅ No explicit system identification needed\n",
    "- ✅ Flexible architecture for complex dynamics\n",
    "- ⚠️ Black box: hard to understand why it works\n",
    "- ⚠️ Requires large training dataset\n",
    "- ⚠️ Can be slow for long sequences\n",
    "\n",
    "**SSM Controller:**\n",
    "- ✅ Theoretically grounded (continuous dynamics → discretization)\n",
    "- ✅ Recursive view efficient for inference\n",
    "- ✅ Convolutional view enables parallel training\n",
    "- ✅ Balance between interpretability and learning capacity\n",
    "- ⚠️ Still requires hyperparameter tuning (state dimension, initialization)\n",
    "- ⚠️ Intermediate complexity between LQR and LSTM\n",
    "\n",
    "### Decision Framework\n",
    "\n",
    "**Choose LQR if:**\n",
    "1. You have accurate system identification (or domain knowledge of A, B matrices)\n",
    "2. The system is approximately linear\n",
    "3. Real-time performance is critical\n",
    "4. Interpretability is important\n",
    "5. Training data is limited\n",
    "\n",
    "**Choose LSTM if:**\n",
    "1. The system has significant nonlinearities\n",
    "2. You have abundant training data\n",
    "3. Black-box solutions are acceptable\n",
    "4. You want to avoid system identification\n",
    "5. Moderate computational resources available\n",
    "\n",
    "**Choose SSM if:**\n",
    "1. You want theoretical grounding with learning flexibility\n",
    "2. You have medium-sized training datasets\n",
    "3. You need both training parallelization and inference speed\n",
    "4. Interpretability is somewhat important\n",
    "5. You want to bridge classical control and deep learning\n",
    "\n",
    "### Hybrid Approach Recommendation\n",
    "\n",
    "**For Production Control Systems:**\n",
    "\n",
    "```\n",
    "1. Start with LQR using system identification (BONUS section)\n",
    "   → Fast baseline, understand system behavior\n",
    "   \n",
    "2. If tracking error is acceptable → use LQR\n",
    "   → Simple, fast, interpretable\n",
    "   \n",
    "3. If performance gaps appear → add LSTM or SSM\n",
    "   → Learn residuals: e = actual_error - predicted_error\n",
    "   → Use NN to predict correction: u_correction = NN(state)\n",
    "   → Combined control: u_total = u_LQR + u_correction\n",
    "   \n",
    "4. Monitor continuously\n",
    "   → If new dynamics appear, retrain LSTM/SSM\n",
    "   → Update LQR if system changes significantly\n",
    "```\n",
    "\n",
    "**Benefits of Hybrid Approach:**\n",
    "- Fast nominal control from LQR\n",
    "- Handles nonlinearities/disturbances with neural network\n",
    "- Interpretable core (LQR) with learned corrections\n",
    "- Graceful degradation if NN fails\n",
    "\n",
    "### Code Organization Summary\n",
    "\n",
    "```\n",
    "Section 1-2:  Data Loading & Preprocessing\n",
    "Section 3-4:  Forward LSTM Model (System Dynamics Learning)\n",
    "Section 5-6:  LQR Controller (Theoretical, Hardcoded A, B)\n",
    "Section 7-8:  LSTM Inverse Model & Closed-Loop Simulation\n",
    "Section 9:    Performance Comparison (LQR vs LSTM)\n",
    "Section 10:   Summary & Architecture Discussion\n",
    "BONUS:        System Identification (Extract A, B from data)\n",
    "              LQR with Identified Matrices\n",
    "Section 11:   State Space Models Theory & Math Formulation\n",
    "              SSM Implementation (Discretization, Recursive, Convolutional)\n",
    "              SSM Training with Learnable Parameters\n",
    "              Recursive vs Convolutional View Comparison\n",
    "Section 12:   Three-Method Comparison (LQR vs LSTM vs SSM)\n",
    "              Performance Metrics, Visualizations\n",
    "              Decision Framework, Hybrid Approaches\n",
    "```\n",
    "\n",
    "### Key Mathematical Insights\n",
    "\n",
    "**1. Discretization via Trapezoid Method:**\n",
    "\n",
    "The continuous SSM:\n",
    "$$\\dot{x}(t) = Ax(t) + Bu(t), \\quad y(t) = Cx(t)$$\n",
    "\n",
    "is discretized to:\n",
    "$$\\bar{A} = (I - \\frac{\\Delta}{2}A)^{-1}(I + \\frac{\\Delta}{2}A)$$\n",
    "$$\\bar{B} = (I - \\frac{\\Delta}{2}A)^{-1}\\Delta B$$\n",
    "\n",
    "This bilinear transformation preserves stability: if A is Hurwitz (Re{λ} < 0), then $|\\bar{A}|$ eigenvalues lie inside unit circle.\n",
    "\n",
    "**2. Recursive vs Convolutional Equivalence:**\n",
    "\n",
    "Both views compute identical outputs:\n",
    "- **Recursive:** $x_k = \\bar{A}x_{k-1} + \\bar{B}u_k$ (sequential, memory efficient)\n",
    "- **Convolutional:** $y_k = \\sum_{j=0}^{k} K_j u_{k-j}$ where $K_j = C\\bar{A}^j\\bar{B}$ (parallel, compute intensive)\n",
    "\n",
    "Trade-off: Recursion is fast for inference (one step at a time), convolution is fast for training (entire batch at once).\n",
    "\n",
    "**3. System Identification as Constraint:**\n",
    "\n",
    "All methods learn the same underlying dynamics. The methods differ in how they represent this knowledge:\n",
    "- **LQR:** Explicit matrices (A, B, K)\n",
    "- **LSTM:** Implicit in weights of two LSTM layers\n",
    "- **SSM:** Learned continuous matrices (A, B, C) with discretization\n",
    "\n",
    "Quality of learning depends on model capacity relative to system complexity.\n",
    "\n",
    "### Final Recommendations\n",
    "\n",
    "1. **For Educational Understanding:** Start with LQR → System ID → LSTM → SSM\n",
    "   - Each builds on previous understanding\n",
    "   - Progression from theory → data-driven → learning\n",
    "\n",
    "2. **For Production Deployment:** LQR + System ID for baseline, then evaluate LSTM/SSM for improvements\n",
    "\n",
    "3. **For Research:** Explore SSM as emerging approach\n",
    "   - State-space models are foundation of modern deep learning (Mamba, S4 architectures)\n",
    "   - Combines classical control rigor with deep learning flexibility\n",
    "   - Active research area with improvements coming regularly\n",
    "\n",
    "---\n",
    "\n",
    "**Created:** State Space Controller Notebook v2.0\n",
    "**Framework:** Three-Paradigm Control Comparison (Classical → Neural → Modern DL)\n",
    "**Data:** Experimental drone control dataset (bdd_in_mat_05.csv)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
